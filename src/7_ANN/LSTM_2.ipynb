{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"../../data/\"\n",
    "#stats stuff\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "\n",
    "# ML stuff\n",
    "import numpy as np\n",
    "from numpy.fft import *\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Lasso\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "\n",
    "# DL stuff\n",
    "from torch.autograd import Variable\n",
    "from fastprogress import master_bar, progress_bar\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# basic stuff\n",
    "import datetime\n",
    "import requests\n",
    "import io\n",
    "from collections import Counter\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set index as datetime\n",
    "def date_index_nasdaq(nasdaq):\n",
    "    nasdaq_c = nasdaq.copy()\n",
    "    dates = pd.to_datetime(nasdaq_c.Date)\n",
    "    nasdaq_c.set_index(dates, inplace=True)\n",
    "    # set date as index\n",
    "    nasdaq_c.drop(\"Date\", axis=1, inplace=True)\n",
    "    nasdaq_c = nasdaq_c[\"2012-05-18\":]\n",
    "    return nasdaq_c\n",
    "\n",
    "############## REINDEX FUNCTION AND PREPARE_STOCK FUNCTION ARE PRETTY MUCH SAME, HOWEVER, I PREFER THE PRIOR ##################\n",
    "# for ARIMA or some shit    \n",
    "def reindex(df):\n",
    "    return df.reindex(pd.date_range(df.index[0], df.index[-1])).fillna(method=\"ffill\")\n",
    "\n",
    "# for prepare_stock\n",
    "def date_range_df(start, end, column_name = \"Time\"):\n",
    "    date_range = pd.date_range(start, end)\n",
    "    df = pd.DataFrame(date_range, columns = [column_name])\n",
    "    df.set_index(column_name, inplace=True)\n",
    "    return df\n",
    "\n",
    "# merging with date range df\n",
    "def prepare_stock(nasdaq, start, end, stock_name=\"AAPL\", drop=True):\n",
    "    nasdaq = nasdaq.loc[nasdaq[\"Name\"]==stock_name]\n",
    "    dates = date_range_df(start, end)\n",
    "    new_nasdaq = dates.merge(nasdaq, how=\"left\", left_index=True, right_index=True)\n",
    "    if drop:\n",
    "        new_nasdaq.dropna(inplace=True)\n",
    "    return new_nasdaq\n",
    "#############################################################################################################################\n",
    "\n",
    "# create features volatility, volume, adj close\n",
    "def get_features(nasdaq):\n",
    "    #rename Adj Close\n",
    "    nasdaq.rename(columns={\"Adj Close\":\"Adj_Close\"}, inplace=True)\n",
    "    nasdaq[\"log_Volatility\"] = np.log(nasdaq.High - nasdaq.Low + 1)\n",
    "    nasdaq[\"log_Volume\"] = np.log(nasdaq.Volume + 1) \n",
    "    nasdaq[\"log_Adj_Close\"] = np.log(nasdaq[\"Adj_Close\"] + 1)\n",
    "    # nasdaq[\"log_Adj_Close_diff\"] = nasdaq[\"log_Adj_Close\"].diff()\n",
    "    nasdaq.drop(columns = [\"Low\", \"High\", \"Close\", \"Open\", \"Name\", \"Volume\"], inplace=True)\n",
    "    # nasdaq.dropna(inplace = True)\n",
    "    return nasdaq\n",
    "\n",
    "# this will return feature engineered stock dataframe\n",
    "def get_stock(nasdaq, stock_name=\"AAPL\"):\n",
    "    nasdaq_c = date_index_nasdaq(nasdaq)\n",
    "    stock = prepare_stock(nasdaq_c, nasdaq_c.index[0], nasdaq_c.index[-1], stock_name)\n",
    "    stock = get_features(stock)\n",
    "    stock.fillna(\"ffill\", inplace=True)\n",
    "    return stock\n",
    "\n",
    "# plot heatmap for top stocks\n",
    "def plot_attribute(nasdaq, using,feature=\"log_Adj_Close\"):\n",
    "    stocks = pd.DataFrame()\n",
    "    for name in using:\n",
    "        stocks[name] = get_stock(nasdaq, name)[feature]\n",
    "    stocks.dropna(inplace=True)\n",
    "    stocks.plot()\n",
    "    plt.show()\n",
    "\n",
    "####### In the 2 functions below, we are adding weekday however ###########\n",
    "####### prob we could have done this in like get_stock or something #######\n",
    "# the main difference between the two is , the prior is just adding weekday at the end,\n",
    "# whereas the latter function is adding it to every stock\n",
    "def get_train_df(nasdaq, using, features):\n",
    "    df_features_arr = reindex(get_stock(nasdaq, using[0])).to_numpy().T\n",
    "    for name in using[1:]:\n",
    "        adding = reindex(get_stock(nasdaq, name)).to_numpy().T\n",
    "        df_features_arr = np.concatenate([df_features_arr, adding])\n",
    "    df_features_arr = df_features_arr.T\n",
    "\n",
    "    ## df_features = pd.DataFrame(data=df_features_arr, columns=pd.MultiIndex.from_tuples(zip(col_one, col_two)))\n",
    "    \n",
    "    # making columns\n",
    "    # features must not include weekday here\n",
    "    if \"weekday\" in features:\n",
    "        features.remove(\"weekday\")\n",
    "    col_one = []\n",
    "    for element in using:\n",
    "        for i in range(len(features)):\n",
    "            col_one.append(element)\n",
    "    col_two = list(features)*len(using)\n",
    "    print(len(col_one), len(col_two))\n",
    "    # scaling \n",
    "    scaler = MinMaxScaler((-1, 1))\n",
    "    scaled = scaler.fit_transform(df_features_arr)\n",
    "    df_features = pd.DataFrame(data=scaled, columns=pd.MultiIndex.from_tuples(zip(col_one, col_two)))\n",
    "\n",
    "    df_features.index = pd.date_range(\"2012-05-18\", \"2021-09-10\")\n",
    "\n",
    "    day_of_week = np.array(list(map(lambda date: date.weekday(), df_features.index)))\n",
    "    day_of_week = day_of_week.reshape(-1, 1)\n",
    "    day_of_week = pd.Series(data=scaler.fit_transform(day_of_week).reshape(-1,), index = df_features.index)\n",
    "    df_features[\"weekday\"] = day_of_week\n",
    "    if \"weekday\" not in features:\n",
    "        features.append(\"weekday\")\n",
    "\n",
    "    return df_features, features\n",
    "\n",
    "\n",
    "# for feeding into network\n",
    "def get_train_arr(nasdaq, using, features):\n",
    "    df_features_arr = []\n",
    "    for name in using:\n",
    "        arr = reindex(get_stock(nasdaq, name)).to_numpy()\n",
    "        # scaling for each column, for each stock_df in nasdaq\n",
    "        scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "        arr_scaled = scaler.fit_transform(arr)    \n",
    "\n",
    "        # adding day of week\n",
    "        day_of_week = np.array(list(map(lambda date: date.weekday(), pd.date_range(\"2012-05-18\", \"2021-09-10\"))))\n",
    "        day_of_week = day_of_week.reshape(-1, 1)\n",
    "        day_of_week = scaler.fit_transform(day_of_week)\n",
    "      \n",
    "        arr_scaled = np.concatenate([arr_scaled, day_of_week], axis=1)\n",
    "\n",
    "        df_features_arr.append(arr_scaled)\n",
    "\n",
    "\n",
    "    df_features_arr = np.array(df_features_arr)\n",
    "    if \"weekday\" not in features:\n",
    "        features.append(\"weekday\")\n",
    "    df_features_arr = df_features_arr.reshape(-1, len(features), 7)\n",
    "\n",
    "    return df_features_arr, features\n",
    "\n",
    "\n",
    "def sliding_windows_mutli_features(data, seq_length, target_cols_ids):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for i in range((data.shape[0])-seq_length-1):\n",
    "        #change here after finishing feature engineering process\n",
    "        _x = data[i:(i+seq_length), :] \n",
    "        _y = data[i+seq_length, target_cols_ids] ## column 1 contains the labbel(log_Adj_Close)\n",
    "        x.append(_x)\n",
    "        y.append(_y)\n",
    "\n",
    "    return np.array(x), np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### LOAD DATA ######################\n",
    "\n",
    "nasdaq = pd.read_csv(data_root + \"NASDAQ_100_Data_From_2010.csv\", sep=\"\\t\")\n",
    "\n",
    "features = ['Adj_Close', 'log_Volatility', 'log_Volume', 'log_Adj_Close']\n",
    "using = ['FB', 'TSLA', 'AAPL', 'AMZN', 'NVDA', 'MSFT', 'GOOGL']\n",
    "# AAPL(Apple), MSFT(Microsoft), GOOGL(Google), AMZN(Amazon), TSLA(Tesla), FB(Facebook), NVDA(Nvidia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 28\n"
     ]
    },
    {
     "data": {
      "text/plain": "((3372, 30, 22), (3372, 7))"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this part is sketchy\n",
    "df, features = get_train_df(nasdaq, using, features)\n",
    "\n",
    "adj_close_cols_ids = []\n",
    "log_adj_close_cols_ids = []\n",
    "volatility_cols_ids = []\n",
    "volume_cols_ids = []\n",
    "weekday_col_id = []\n",
    "count = 0\n",
    "for col in df.columns:\n",
    "    if col[1] == \"Adj_Close\":\n",
    "        df.drop(col, axis=1, inplace=True)\n",
    "        count -= 1\n",
    "    if col[1] == \"log_Adj_Close\":\n",
    "        adj_close_cols_ids.append(count)\n",
    "    if col[1] == \"log_Volume\":\n",
    "        volume_cols_ids.append(count)\n",
    "    if col[1] == \"log_Volatility\":\n",
    "        volatility_cols_ids.append(count)\n",
    "    if col[0] == \"weekday\":\n",
    "        weekday_col_id.append(count)\n",
    "    count += 1\n",
    "df = df.to_numpy()\n",
    "x, y = sliding_windows_mutli_features(df, 30, adj_close_cols_ids)\n",
    "\n",
    "x.shape, y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape is: torch.Size([2697, 30, 22])\n",
      "train label shape is: torch.Size([2697, 7])\n",
      "test shape is: torch.Size([675, 30, 22])\n",
      "test label shape is: torch.Size([675, 7])\n"
     ]
    }
   ],
   "source": [
    "# train test split (70:30)\n",
    "train_size = int(len(y)*0.80)\n",
    "test_size = len(y) - train_size\n",
    "\n",
    "dataX = Variable(torch.Tensor(np.array(x)))\n",
    "dataY = Variable(torch.Tensor(np.array(y)))\n",
    "\n",
    "trainX = Variable(torch.Tensor(np.array(x[0:train_size])))\n",
    "trainY = Variable(torch.Tensor(np.array(y[0:train_size])))\n",
    "\n",
    "testX = Variable(torch.Tensor(np.array(x[train_size:len(x)])))\n",
    "testY = Variable(torch.Tensor(np.array(y[train_size:len(y)])))\n",
    "\n",
    "print(\"train shape is:\",trainX.size())\n",
    "print(\"train label shape is:\",trainY.size())\n",
    "print(\"test shape is:\",testX.size())\n",
    "print(\"test label shape is:\",testY.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "class LSTM2(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n",
    "        super(LSTM2, self).__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        #self.seq_length = seq_length\n",
    "\n",
    "        \n",
    "        # what does the batch_first do\n",
    "        self.LSTM2 = nn.LSTM(\\\n",
    "            input_size=input_size, \n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers, \n",
    "            batch_first=True,\n",
    "            dropout = 0.25)\n",
    "        \n",
    "        # Linear(in_features, out_features)\n",
    "        self.fc1 = nn.Linear(hidden_size, 256)                                                                                                                                                                                                                           \n",
    "        self.bn1 = nn.BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.dp1 = nn.Dropout(0.25)\n",
    "\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.dp2 = nn.Dropout(0.2)\n",
    "\n",
    "        self.fc3 = nn.Linear(128, 7)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_1 = Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size).to(device))\n",
    "        \n",
    "        c_1 = Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size).to(device))\n",
    "        \n",
    "        # Propagate input through LSTM\n",
    "        _, (hn, cn) = self.LSTM2(x, (h_1, c_1))\n",
    "        y = hn.view(-1, self.hidden_size)\n",
    "\n",
    "        final_state = hn.view(self.num_layers, x.size(0), self.hidden_size)[-1]\n",
    "\n",
    "        x0 = self.fc1(final_state)\n",
    "        x0 = self.bn1(x0)\n",
    "        x0 = self.dp1(x0)\n",
    "        x0 = self.relu(x0)\n",
    "\n",
    "        x0 = self.fc2(x0)\n",
    "        x0 = self.bn2(x0)\n",
    "        x0 = self.dp2(x0)\n",
    "\n",
    "        x0 = self.relu(x0)\n",
    "        \n",
    "        out = self.fc3(x0)                                         \n",
    "        # out = self.dropout(out)\n",
    "       \n",
    "        return out\n",
    "\n",
    "def init_weights(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.88, 0.08)\n",
    "\n",
    "# create a nn class (just-for-fun choice :-) \n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "    def forward(self,yhat,y):\n",
    "        return torch.sqrt(self.mse(yhat,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Parameters #######\n",
    "num_epochs = 700\n",
    "learning_rate = 1e-3\n",
    "input_size = 22 # feature nums\n",
    "hidden_size = 512\n",
    "num_layers = 2\n",
    "num_classes = 7 # because we are using 7 stocks\n",
    "#########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2697, 30, 22])"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n                background: #F44336;\n            }\n        </style>\n      <progress value='291' class='' max='700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      41.57% [291/700 1:23:39<1:57:34]\n    </div>\n    ",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 244.97987365722656, valid loss:256754.15625\n",
      "Epoch: 50, loss: 137.4081573486328, valid loss:619.5578002929688\n",
      "Epoch: 100, loss: 81.82657623291016, valid loss:353.4774475097656\n",
      "Epoch: 150, loss: 51.77170944213867, valid loss:220.6194610595703\n",
      "Epoch: 200, loss: 33.650917053222656, valid loss:142.38873291015625\n",
      "saved best model epoch: 243 val loss is: 99.82648468017578\n",
      "saved best model epoch: 244 val loss is: 99.05354309082031\n",
      "saved best model epoch: 245 val loss is: 98.2851333618164\n",
      "saved best model epoch: 246 val loss is: 98.07386779785156\n",
      "saved best model epoch: 247 val loss is: 97.12197875976562\n",
      "saved best model epoch: 248 val loss is: 96.6666259765625\n",
      "saved best model epoch: 249 val loss is: 95.70043182373047\n",
      "saved best model epoch: 250 val loss is: 95.17382049560547\n",
      "Epoch: 250, loss: 23.385440826416016, valid loss:95.17382049560547\n",
      "saved best model epoch: 251 val loss is: 94.39668273925781\n",
      "saved best model epoch: 252 val loss is: 93.59685516357422\n",
      "saved best model epoch: 253 val loss is: 92.87239074707031\n",
      "saved best model epoch: 254 val loss is: 92.34690856933594\n",
      "saved best model epoch: 255 val loss is: 91.87385559082031\n",
      "saved best model epoch: 256 val loss is: 91.40632629394531\n",
      "saved best model epoch: 257 val loss is: 90.79702758789062\n",
      "saved best model epoch: 258 val loss is: 90.12247467041016\n",
      "saved best model epoch: 259 val loss is: 89.53453063964844\n",
      "saved best model epoch: 260 val loss is: 88.8607177734375\n",
      "saved best model epoch: 261 val loss is: 88.3212890625\n",
      "saved best model epoch: 262 val loss is: 87.56903076171875\n",
      "saved best model epoch: 263 val loss is: 86.93800354003906\n",
      "saved best model epoch: 264 val loss is: 86.5986328125\n",
      "saved best model epoch: 265 val loss is: 86.01139831542969\n",
      "saved best model epoch: 266 val loss is: 85.39432525634766\n",
      "saved best model epoch: 267 val loss is: 85.01209259033203\n",
      "saved best model epoch: 268 val loss is: 84.67385864257812\n",
      "saved best model epoch: 269 val loss is: 84.25629425048828\n",
      "saved best model epoch: 270 val loss is: 83.58248901367188\n",
      "saved best model epoch: 271 val loss is: 82.89301300048828\n",
      "saved best model epoch: 272 val loss is: 82.43841552734375\n",
      "saved best model epoch: 273 val loss is: 81.77316284179688\n",
      "saved best model epoch: 274 val loss is: 81.26764678955078\n",
      "saved best model epoch: 275 val loss is: 80.73701477050781\n",
      "saved best model epoch: 276 val loss is: 80.0084228515625\n",
      "saved best model epoch: 277 val loss is: 79.47793579101562\n",
      "saved best model epoch: 278 val loss is: 79.02159881591797\n",
      "saved best model epoch: 279 val loss is: 78.48220825195312\n",
      "saved best model epoch: 280 val loss is: 77.71688842773438\n",
      "saved best model epoch: 281 val loss is: 77.19910430908203\n",
      "saved best model epoch: 282 val loss is: 76.63724517822266\n",
      "saved best model epoch: 283 val loss is: 76.2904052734375\n",
      "saved best model epoch: 284 val loss is: 75.60955810546875\n",
      "saved best model epoch: 285 val loss is: 75.08938598632812\n",
      "saved best model epoch: 286 val loss is: 74.36289978027344\n",
      "saved best model epoch: 287 val loss is: 73.63287353515625\n",
      "saved best model epoch: 288 val loss is: 72.92296600341797\n",
      "saved best model epoch: 289 val loss is: 72.77510070800781\n",
      "saved best model epoch: 290 val loss is: 72.08648681640625\n"
     ]
    }
   ],
   "source": [
    "############################################################################################\n",
    "############################################################################################\n",
    "############################### ONLY RUN FOR TRAINING ######################################\n",
    "############################################################################################\n",
    "############################################################################################\n",
    "best_val_loss = 100 \n",
    "### Init Model\n",
    "lstm = LSTM2(num_classes, input_size, hidden_size, num_layers)\n",
    "lstm.to(device)\n",
    "lstm.apply(init_weights)\n",
    "\n",
    "### Set Criterion Optimizer and scheduler\n",
    "criterion = torch.nn.MSELoss().to(device) \n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=100, factor=0.5, min_lr=1e-7, eps=1e-08)\n",
    "\n",
    "#optimizer = torch.optim.SGD(lstm.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train model\n",
    "for epoch in progress_bar(range(num_epochs)):\n",
    "    lstm.train()\n",
    "    outputs= lstm(trainX.to(device))\n",
    "    optimizer.zero_grad()\n",
    "    torch.nn.utils.clip_grad_norm_(lstm.parameters(),1)\n",
    "\n",
    "    # obtain loss func\n",
    "    loss = criterion(outputs, trainY.to(device))\n",
    "    loss.backward()\n",
    "\n",
    "    scheduler.step(loss)\n",
    "    optimizer.step()\n",
    "\n",
    "    #evaluate on test\n",
    "    lstm.eval()\n",
    "    valid = lstm(testX.to(device))\n",
    "    vall_loss = criterion(valid, testY.to(device))\n",
    "\n",
    "    scheduler.step(vall_loss)\n",
    "\n",
    "    if vall_loss.cpu().item() < best_val_loss:\n",
    "         torch.save(lstm.state_dict(), 'best_model.pt')\n",
    "         print(\"saved best model epoch:\",epoch,\"val loss is:\",vall_loss.cpu().item())\n",
    "         best_val_loss = vall_loss.cpu().item()\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch: {epoch}, loss: {loss.cpu().item()}, valid loss:{vall_loss.cpu().item()}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX_c = testX.cpu().detach().numpy().copy()\n",
    "\n",
    "# testX_drop_Volatility = np.delete(testX_c, [0, 3, 6, 9, 12, 15, 18], 2)\n",
    "# testX_drop_Volume = np.delete(testX_c, [1, 4, 7, 10, 13, 16, 19], 2)\n",
    "# testX_drop_Adj_Close = np.delete(testX_c, target_cols_ids, 2)\n",
    "testX_drop_Volatility = testX_c.copy()\n",
    "testX_drop_Volatility[:,:,volatility_cols_ids] = 0.\n",
    "testX_drop_Volume = testX_c.copy()\n",
    "testX_drop_Volume[:,:,volume_cols_ids] = 0.\n",
    "testX_drop_Adj_Close = testX_c.copy()\n",
    "testX_drop_Adj_Close[:, :, adj_close_cols_ids] = 0.\n",
    "\n",
    "testX_drop_Volatility = Variable(torch.Tensor(testX_drop_Volatility))\n",
    "testX_drop_Volume = Variable(torch.Tensor(testX_drop_Volume))\n",
    "testX_drop_Adj_Close = Variable(torch.Tensor(testX_drop_Adj_Close))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('pyflux': conda)",
   "name": "python3812jvsc74a57bd0b8bf3bbb95d33652ea8a09e83516ae388afde8f9530fb9552010d87506ab4938"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "metadata": {
   "interpreter": {
    "hash": "b8bf3bbb95d33652ea8a09e83516ae388afde8f9530fb9552010d87506ab4938"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}