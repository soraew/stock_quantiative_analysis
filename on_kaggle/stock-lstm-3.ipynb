{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-01-21T01:09:31.518258Z","iopub.status.busy":"2022-01-21T01:09:31.517733Z","iopub.status.idle":"2022-01-21T01:09:31.557783Z","shell.execute_reply":"2022-01-21T01:09:31.557052Z","shell.execute_reply.started":"2022-01-21T01:09:31.518163Z"},"trusted":true},"outputs":[],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-01-21T01:09:34.590988Z","iopub.status.busy":"2022-01-21T01:09:34.590675Z","iopub.status.idle":"2022-01-21T01:09:38.924386Z","shell.execute_reply":"2022-01-21T01:09:38.923540Z","shell.execute_reply.started":"2022-01-21T01:09:34.590952Z"},"trusted":true},"outputs":[],"source":["data_root = \"/kaggle/input/nasdaq100-stock-price-data/\"\n","#stats stuff\n","from statsmodels.graphics.tsaplots import plot_acf\n","from statsmodels.graphics.tsaplots import plot_pacf\n","\n","# ML stuff\n","import numpy as np\n","from numpy.fft import *\n","import torch\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import TimeSeriesSplit\n","from sklearn.metrics import mean_squared_error\n","from sklearn.linear_model import Lasso\n","import pandas as pd\n","import lightgbm as lgb\n","\n","\n","# DL stuff\n","from torch.autograd import Variable\n","from fastprogress import master_bar, progress_bar\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","\n","\n","# plotting\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","\n","\n","# basic stuff\n","import datetime\n","import requests\n","import io\n","import os\n","from collections import Counter\n","from tqdm import tqdm\n","\n","# relative imports\n","# from LSTM_preprocess import get_Xy, get_train_test, get_train_df, sliding_windows_mutli_features\n","# from hfuncs import check_mkdir"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-01-21T01:09:38.926151Z","iopub.status.busy":"2022-01-21T01:09:38.925901Z","iopub.status.idle":"2022-01-21T01:09:38.970193Z","shell.execute_reply":"2022-01-21T01:09:38.969404Z","shell.execute_reply.started":"2022-01-21T01:09:38.926115Z"},"trusted":true},"outputs":[],"source":["########################################################################\n","################## this cell is dedicated to kaggle ####################\n","########################################################################\n","\n","# set index as datetime\n","def date_index_nasdaq(nasdaq):\n","    nasdaq_c = nasdaq.copy()\n","    dates = pd.to_datetime(nasdaq_c.Date)\n","    nasdaq_c.set_index(dates, inplace=True)\n","    # set date as index\n","    nasdaq_c.drop(\"Date\", axis=1, inplace=True)\n","    # ここでFBとかTESLAとかに合わせている\n","    nasdaq_c = nasdaq_c[\"2012-05-18\":]\n","    return nasdaq_c\n","\n","############## REINDEX FUNCTION AND PREPARE_STOCK FUNCTION ARE PRETTY MUCH SAME, HOWEVER, I PREFER THE PRIOR ##################\n","# for ARIMA or some shit    \n","def reindex(df):\n","    return df.reindex(pd.date_range(df.index[0], df.index[-1])).fillna(method=\"ffill\")\n","\n","# for prepare_stock\n","def date_range_df(start, end, column_name = \"Time\"):\n","    date_range = pd.date_range(start, end)\n","    df = pd.DataFrame(date_range, columns = [column_name])\n","    df.set_index(column_name, inplace=True)\n","    return df\n","\n","# merging with date range df\n","def prepare_stock(nasdaq, start, end, stock_name=\"AAPL\", drop=True):\n","    nasdaq = nasdaq.loc[nasdaq[\"Name\"]==stock_name]\n","    dates = date_range_df(start, end)\n","    new_nasdaq = dates.merge(nasdaq, how=\"left\", left_index=True, right_index=True)\n","    if drop:\n","        new_nasdaq.dropna(inplace=True)\n","    return new_nasdaq\n","#############################################################################################################################\n","\n","# create features volatility, volume, adj close\n","def get_features(nasdaq):\n","    #rename Adj Close\n","    nasdaq.rename(columns={\"Adj Close\":\"Adj_Close\"}, inplace=True)\n","    nasdaq[\"log_Volatility\"] = np.log(nasdaq.High - nasdaq.Low + 1)\n","    nasdaq[\"log_Volume\"] = np.log(nasdaq.Volume + 1) \n","    nasdaq[\"log_Adj_Close\"] = np.log(nasdaq[\"Adj_Close\"] + 1)\n","    # nasdaq[\"log_Adj_Close_diff\"] = nasdaq[\"log_Adj_Close\"].diff()\n","    nasdaq.drop(columns = [\"Low\", \"High\", \"Close\", \"Open\", \"Name\", \"Volume\"], inplace=True)\n","    # nasdaq.dropna(inplace = True)\n","    return nasdaq\n","\n","# this will return feature engineered stock dataframe\n","def get_stock(nasdaq, stock_name=\"AAPL\"):\n","    nasdaq_c = date_index_nasdaq(nasdaq)\n","    stock = prepare_stock(nasdaq_c, nasdaq_c.index[0], nasdaq_c.index[-1], stock_name)\n","    stock = get_features(stock)\n","    stock.fillna(\"ffill\", inplace=True)\n","    return stock\n","\n","# plot heatmap for top stocks\n","def plot_attribute(nasdaq, using,feature=\"log_Adj_Close\"):\n","    stocks = pd.DataFrame()\n","    for name in using:\n","        stocks[name] = get_stock(nasdaq, name)[feature]\n","    stocks.dropna(inplace=True)\n","    stocks.plot()\n","    plt.show()\n","\n","####### In the 2 functions below, we are adding weekday however ###########\n","####### prob we could have done this in like get_stock or something #######\n","# the main difference between the two is , the prior is just adding weekday at the end,\n","# whereas the latter function is adding it to every stock\n","def get_train_df(nasdaq, using, features):\n","    df_features_arr = reindex(get_stock(nasdaq, using[0])).to_numpy().T\n","    for name in using[1:]:\n","        adding = reindex(get_stock(nasdaq, name)).to_numpy().T\n","        df_features_arr = np.concatenate([df_features_arr, adding])\n","    df_features_arr = df_features_arr.T\n","\n","    ## df_features = pd.DataFrame(data=df_features_arr, columns=pd.MultiIndex.from_tuples(zip(col_one, col_two)))\n","    \n","    # making columns\n","    # features must not include weekday here\n","    if \"weekday\" in features:\n","        features.remove(\"weekday\")\n","    col_one = []\n","    for element in using:\n","        for i in range(len(features)):\n","            col_one.append(element)\n","    col_two = list(features)*len(using)\n","    # print(len(col_one), len(col_two))\n","\n","    # scaling \n","    scaler = MinMaxScaler((-1, 1))\n","    scaled = scaler.fit_transform(df_features_arr)\n","    df_features = pd.DataFrame(data=scaled, columns=pd.MultiIndex.from_tuples(zip(col_one, col_two)))\n","\n","    df_features.index = pd.date_range(\"2012-05-18\", \"2021-09-10\")\n","\n","    day_of_week = np.array(list(map(lambda date: date.weekday(), df_features.index)))\n","    day_of_week = day_of_week.reshape(-1, 1)\n","    day_of_week = pd.Series(data=scaler.fit_transform(day_of_week).reshape(-1,), index = df_features.index)\n","    df_features[\"weekday\"] = day_of_week\n","    if \"weekday\" not in features:\n","        features.append(\"weekday\")\n","\n","    return df_features, features\n","\n","\n","# for feeding into network\n","def get_train_arr(nasdaq, using, features):\n","    df_features_arr = []\n","    for name in using:\n","        arr = reindex(get_stock(nasdaq, name)).to_numpy()\n","        # scaling for each column, for each stock_df in nasdaq\n","        scaler = MinMaxScaler(feature_range=(-1, 1))\n","        arr_scaled = scaler.fit_transform(arr)    \n","\n","        # adding day of week\n","        day_of_week = np.array(list(map(lambda date: date.weekday(), pd.date_range(\"2012-05-18\", \"2021-09-10\"))))\n","        day_of_week = day_of_week.reshape(-1, 1)\n","        day_of_week = scaler.fit_transform(day_of_week)\n","      \n","        arr_scaled = np.concatenate([arr_scaled, day_of_week], axis=1)\n","\n","        df_features_arr.append(arr_scaled)\n","\n","\n","    df_features_arr = np.array(df_features_arr)\n","    if \"weekday\" not in features:\n","        features.append(\"weekday\")\n","    df_features_arr = df_features_arr.reshape(-1, len(features), 7)\n","\n","    return df_features_arr, features\n","\n","\n","def sliding_windows_mutli_features(data, seq_length, target_cols_ids):\n","    x = []\n","    y = []\n","\n","    for i in range((data.shape[0])-seq_length-1):\n","        #change here after finishing feature engineering process\n","        _x = data[i:(i+seq_length), :] \n","        _y = data[i+seq_length, target_cols_ids] ## column 1 contains the labbel(log_Adj_Close)\n","        x.append(_x)\n","        y.append(_y)\n","\n","    return np.array(x), np.array(y)\n","\n","def get_Xy(df, window_size):\n","    log_adj_close_cols_ids = []\n","    volatility_cols_ids = []\n","    volume_cols_ids = []\n","    weekday_col_id = []\n","    count = 0\n","    for col in df.columns:\n","        # print(col)\n","        if col[1] == \"Adj_Close\":\n","            df.drop(col, axis=1, inplace=True)\n","            count -= 1\n","        if col[1] == \"log_Adj_Close\":\n","            log_adj_close_cols_ids.append(count)\n","        if col[1] == \"log_Volume\":\n","            volume_cols_ids.append(count)\n","        if col[1] == \"log_Volatility\":\n","            volatility_cols_ids.append(count)\n","        if col[0] == \"weekday\":\n","            weekday_col_id.append(count)\n","        count += 1\n","    df = df.to_numpy()\n","    x, y = sliding_windows_mutli_features(df, window_size, log_adj_close_cols_ids)\n","\n","    # x.shape, y.shape\n","    return x, y\n","\n","def get_train_test(x, y, train_ratio):\n","    train_size = int(len(y)*train_ratio)\n","    test_size = len(y) - train_size\n","\n","    dataX = Variable(torch.Tensor(np.array(x)))\n","    dataY = Variable(torch.Tensor(np.array(y)))\n","\n","    trainX = Variable(torch.Tensor(np.array(x[0:train_size])))\n","    trainY = Variable(torch.Tensor(np.array(y[0:train_size])))\n","\n","    testX = Variable(torch.Tensor(np.array(x[train_size:len(x)])))\n","    testY = Variable(torch.Tensor(np.array(y[train_size:len(y)])))\n","\n","    return trainX, trainY, testX, testY\n","\n","\n","def check_mkdir(dir_name):\n","    if not os.path.exists(dir_name):\n","        os.mkdir(dir_name)\n","\n","def random_search_lgbm(X, y, params_space):\n","    params_log = {}\n","    iteration = 40\n","    cv = TimeSeriesSplit()\n","    for i in tqdm(range(iteration)):\n","        params = {}\n","        for key in params_space.keys():\n","            param_list = params_space[key]\n","            length = len(param_list)\n","            idx =np.random.randint(0,length) \n","            params.update({key:param_list[idx]})\n","            # fit model to data\n","        \n","        model = lgb.LGBMRegressor(**params)\n","        for train_idx, test_idx, in cv.split(X):\n","            X_train, X_test = X[train_idx], X[test_idx]\n","            y_train, y_test = y[train_idx], y[test_idx]\n","            model.fit(X_train, y_train)\n","            y_pred = model.predict(X_test).squeeze()\n","            coef_score = np.corrcoef(y_pred, y_test)[0][1]\n","            rmse_score = np.sqrt(mean_squared_error(y_pred, y_test))\n","        params_log.update({i:[coef_score, rmse_score, params]})\n","\n","    sorted_by_coef=sorted(params_log.items(), key = lambda item: item[1][0], reverse=True)\n","    sorted_by_rmse=sorted(params_log.items(), key = lambda item: item[1][1])\n","    \n","    return sorted_by_coef, sorted_by_rmse"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-01-21T01:09:38.972379Z","iopub.status.busy":"2022-01-21T01:09:38.972076Z","iopub.status.idle":"2022-01-21T01:09:39.764378Z","shell.execute_reply":"2022-01-21T01:09:39.763630Z","shell.execute_reply.started":"2022-01-21T01:09:38.972343Z"},"trusted":true},"outputs":[],"source":["#################### LOAD DATA ######################\n","nasdaq = pd.read_csv(data_root + \"NASDAQ_100_Data_From_2010.csv\", sep=\"\\t\")\n","\n","window_size = 50\n","train_ratio = 0.80\n","device = torch.device('cuda')\n","features = ['Adj_Close', 'log_Volatility', 'log_Volume', 'log_Adj_Close']\n","using = ['FB', 'TSLA', 'AAPL', 'AMZN', 'NVDA', 'MSFT', 'GOOGL']\n","# AAPL(Apple), MSFT(Microsoft), GOOGL(Google), AMZN(Amazon), TSLA(Tesla), FB(Facebook), NVDA(Nvidia)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-01-21T01:09:39.766720Z","iopub.status.busy":"2022-01-21T01:09:39.766463Z","iopub.status.idle":"2022-01-21T01:09:39.943539Z","shell.execute_reply":"2022-01-21T01:09:39.942735Z","shell.execute_reply.started":"2022-01-21T01:09:39.766676Z"},"trusted":true},"outputs":[],"source":["aapl = get_stock(nasdaq)\n","train_size = int(len(aapl)*train_ratio)\n","# train_aapl = aapl[:train_size]\n","# test_aapl = aapl[train_size:]\n","y = aapl[[\"log_Adj_Close\"]].shift(-1).dropna().to_numpy().squeeze()\n","X = aapl.drop(columns=[\"Adj_Close\"]).to_numpy()[:-1]\n","X_train, X_test = X[:train_size], X[train_size:]\n","y_train, y_test = y[:train_size], y[train_size:]\n","\n","X_train.shape, y_train.shape"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-01-21T01:09:39.945173Z","iopub.status.busy":"2022-01-21T01:09:39.944852Z","iopub.status.idle":"2022-01-21T01:09:45.064376Z","shell.execute_reply":"2022-01-21T01:09:45.063365Z","shell.execute_reply.started":"2022-01-21T01:09:39.945135Z"},"trusted":true},"outputs":[],"source":["params_space = {\n","        \"max_depth\":[4, 5, 6, 7, 8, 9],\n","        \"min_data_in_leaf\":[15, 20, 25],\n","        \"learning_rate\":[0.03, 0.04, 0.05, 0.1, 0.01, 0.005],\n","        \"num_leaves\":[25, 30, 35, 40],\n","        \"boosting_type\":[\"gbdt\"],\n","        \"objective\":[\"regression\"],\n","        \"random_state\":[2021],\n","        \"reg_alpha\":[0.1,0.01, 1, 102],\n","        \"reg_lambda\":[0.1, 0.01, 1, 1.2, 1.4]\n","        }\n","sorted_by_coef, sorted_by_rmse = random_search_lgbm(X, y, params_space)\n","sorted_by_coef[0], sorted_by_rmse[0]"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-01-21T01:09:45.066192Z","iopub.status.busy":"2022-01-21T01:09:45.065920Z","iopub.status.idle":"2022-01-21T01:09:45.424798Z","shell.execute_reply":"2022-01-21T01:09:45.424072Z","shell.execute_reply.started":"2022-01-21T01:09:45.066146Z"},"trusted":true},"outputs":[],"source":["\n","\n","# random_search_lgbm(X, y, params_space)\n","\n","\n","params1 = {'max_depth': 9,\n","    'min_data_in_leaf': 15,\n","    'learning_rate': 0.05,\n","    'num_leaves': 35,\n","    'boosting_type': 'gbdt',\n","    'objective': 'regression',\n","    'random_state': 2021,\n","    'reg_alpha': 0.1,\n","    'reg_lambda': 1}\n","\n","params2 = {'max_depth': 6,\n","#     'min_data_in_leaf': 15,\n","    'learning_rate': 0.01,\n","    'num_leaves': 30,\n","    'boosting_type': 'gbdt',\n","    'objective': 'regression',\n","    'random_state': 2021,\n","    'reg_alpha': 1,\n","    'reg_lambda': 1.2}\n","\n","\n","\n","model1 = lgb.LGBMRegressor()\n","model2 = lgb.LGBMRegressor()\n","model1.fit(X_train, y_train)\n","model2.fit(X_train, y_train)\n","pred1 = model1.predict(X_test)\n","pred2 = model2.predict(X_test)\n","plt.plot(y_test)\n","plt.plot(pred1)\n","plt.plot(pred2)\n","plt.legend([\"GT\", \"pred1\", \"pred2\"])"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-01-21T01:09:45.426473Z","iopub.status.busy":"2022-01-21T01:09:45.426224Z","iopub.status.idle":"2022-01-21T01:09:45.438833Z","shell.execute_reply":"2022-01-21T01:09:45.438009Z","shell.execute_reply.started":"2022-01-21T01:09:45.426437Z"},"trusted":true},"outputs":[],"source":["pred1"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-01-21T01:09:45.441057Z","iopub.status.busy":"2022-01-21T01:09:45.440551Z","iopub.status.idle":"2022-01-21T01:09:45.448595Z","shell.execute_reply":"2022-01-21T01:09:45.447768Z","shell.execute_reply.started":"2022-01-21T01:09:45.441016Z"},"trusted":true},"outputs":[],"source":["sorted_by_coef[0], sorted_by_rmse[0]"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-01-21T01:09:45.451831Z","iopub.status.busy":"2022-01-21T01:09:45.451519Z","iopub.status.idle":"2022-01-21T01:09:46.488624Z","shell.execute_reply":"2022-01-21T01:09:46.487845Z","shell.execute_reply.started":"2022-01-21T01:09:45.451795Z"},"trusted":true},"outputs":[],"source":["df, features = get_train_df(nasdaq, using, features)\n","x, y = get_Xy(df, window_size)\n","print('x.shape, y.shape',x.shape, y.shape)\n","trainX, trainY, testX, testY = get_train_test(x, y, train_ratio)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-01-21T01:09:46.490559Z","iopub.status.busy":"2022-01-21T01:09:46.489854Z","iopub.status.idle":"2022-01-21T01:09:46.502742Z","shell.execute_reply":"2022-01-21T01:09:46.502065Z","shell.execute_reply.started":"2022-01-21T01:09:46.490519Z"},"trusted":true},"outputs":[],"source":["class LSTM(nn.Module):\n","\n","    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n","        super(LSTM, self).__init__()\n","        \n","        self.num_classes = num_classes\n","        self.num_layers = num_layers\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        #self.seq_length = seq_length\n","\n","        self.dropout = nn.Dropout(p=0.2)\n","        \n","        # what does the batch_first do\n","        self.lstm = nn.LSTM(\\\n","            input_size=input_size, \n","            hidden_size=hidden_size,\n","            num_layers=num_layers, \n","            batch_first=True,\n","            dropout = 0.25)\n","        \n","        # Linear(in_features, out_features)\n","        self.fc = nn.Linear(hidden_size, num_classes)                                                                                                                                                                                                                           \n","\n","    def forward(self, x):\n","        h_0 = Variable(torch.zeros(\n","            self.num_layers, x.size(0), self.hidden_size).to(device))\n","        \n","        c_0 = Variable(torch.zeros(\n","            self.num_layers, x.size(0), self.hidden_size).to(device))\n","        \n","        # Propagate input through LSTM\n","        ula, (h_out, _) = self.lstm(x, (h_0, c_0))\n","        \n","        h_out = h_out.view(-1, self.hidden_size)\n","        \n","        out = self.fc(h_out)                                         \n","        out = self.dropout(out)\n","       \n","        return out\n","\n","# create a nn class (just-for-fun choice :-) \n","class RMSELoss(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.mse = nn.MSELoss()\n","        \n","    def forward(self,yhat,y):\n","        return torch.sqrt(self.mse(yhat,y))\n","\n","\n","class PearsonLoss(nn.Module):\n","    def forward(self, x, y):\n","        vx = x - torch.mean(x)\n","        vy = y - torch.mean(y)\n"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-01-21T01:09:46.505010Z","iopub.status.busy":"2022-01-21T01:09:46.504474Z","iopub.status.idle":"2022-01-21T01:09:46.515914Z","shell.execute_reply":"2022-01-21T01:09:46.515228Z","shell.execute_reply.started":"2022-01-21T01:09:46.504969Z"},"trusted":true},"outputs":[],"source":["###### Parameters #######\n","num_epochs = 900\n","learning_rate = 1e-3\n","input_size = 22 # features(?)\n","hidden_size = 512\n","num_layers = 1\n","num_classes = 7 # because we are using 7 stocks\n","#########################\n","best_val_loss = 100                        "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-01-21T01:09:46.517533Z","iopub.status.busy":"2022-01-21T01:09:46.517259Z"},"trusted":true},"outputs":[],"source":["######################################## ONLY RUN FOR TRAINING ######################################\n","### Init Model\n","lstm = LSTM(num_classes, input_size, hidden_size, num_layers)\n","lstm.to(device)\n","\n","### where to save models\n","check_mkdir(\"LSTM_3_out\")\n","\n","### Set Criterion Optimizer and scheduler\n","criterion = torch.nn.MSELoss().to(device) \n","optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate, weight_decay=1e-5)\n","\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=500, factor=0.5, min_lr=1e-7, eps=1e-08)\n","\n","#optimizer = torch.optim.SGD(lstm.parameters(), lr=learning_rate)\n","\n","# Train model\n","for epoch in progress_bar(range(num_epochs)):\n","    lstm.train()\n","    outputs= lstm(trainX.to(device))\n","    optimizer.zero_grad()\n","\n","    # obtain loss func\n","    loss = criterion(outputs, trainY.to(device))\n","    loss.backward()\n","\n","    optimizer.step()\n","\n","    #evaluate on test\n","    lstm.eval()\n","    valid = lstm(testX.to(device))\n","    vall_loss = criterion(valid, testY.to(device))\n","\n","    scheduler.step(vall_loss)\n","\n","    if vall_loss.cpu().item() < best_val_loss:\n","        torch.save(lstm.state_dict(), 'best_model.pt')\n","        print(\"saved best model epoch:\",epoch,\"val loss is:\",vall_loss.cpu().item())\n","        best_val_loss = vall_loss.cpu().item()\n","\n","    if epoch%50==0:\n","        print(f\"Epoch: {epoch}, loss: {loss.cpu().item()}, valid loss:{vall_loss.cpu().item()}\")\n","\n","\n","# check_mkdir(\"LSTM_3_out\")\n","# torch.save(lstm.state_dict(), 'LSTM_3_out/model1_1.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.figure(figsize=(13, 7))\n","lstm.eval()\n","pred = lstm(testX[:, :, :].to(device)).cpu().detach().numpy()\n","plt.plot(testY[:, :].cpu(), c=\"blue\")\n","# plt.plot(np.exp(0.1+ pred)-1., c=\"red\")\n","plt.plot(pred, c=\"red\")\n","plt.savefig(f\"pred_v_val_epochs{num_epochs}.png\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.7 64-bit ('base': conda)","name":"python3812jvsc74a57bd0d2be0c264441de96883e4724031315de599358b76b69c68545da3d61c7404889"},"language_info":{"name":"python","version":""},"metadata":{"interpreter":{"hash":"359a4b97f068d7ed9d004d4ac819bdb60a7f87a20daf98c99b93ec32248dbfaf"}}},"nbformat":4,"nbformat_minor":4}