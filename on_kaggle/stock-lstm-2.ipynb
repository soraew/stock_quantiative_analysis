{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2022-01-21T01:49:15.910944Z","iopub.execute_input":"2022-01-21T01:49:15.911658Z","iopub.status.idle":"2022-01-21T01:49:15.946176Z","shell.execute_reply.started":"2022-01-21T01:49:15.911533Z","shell.execute_reply":"2022-01-21T01:49:15.945378Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"data_root = \"/kaggle/input/nasdaq100-stock-price-data/\"\n#stats stuff\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\n\n# ML stuff\nimport numpy as np\nfrom numpy.fft import *\nimport torch\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import Lasso\nimport pandas as pd\nimport lightgbm as lgb\n\n# DL stuff\nfrom torch.autograd import Variable\nfrom fastprogress import master_bar, progress_bar\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\n\n\n# plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# basic stuff\nimport datetime\nimport requests\nimport io\nfrom collections import Counter\n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-21T01:49:15.947929Z","iopub.execute_input":"2022-01-21T01:49:15.948188Z","iopub.status.idle":"2022-01-21T01:49:20.511206Z","shell.execute_reply.started":"2022-01-21T01:49:15.948154Z","shell.execute_reply":"2022-01-21T01:49:20.510484Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Data loading and preprocessing functions","metadata":{}},{"cell_type":"code","source":"# set index as datetime\ndef date_index_nasdaq(nasdaq):\n    nasdaq_c = nasdaq.copy()\n    dates = pd.to_datetime(nasdaq_c.Date)\n    nasdaq_c.set_index(dates, inplace=True)\n    # set date as index\n    nasdaq_c.drop(\"Date\", axis=1, inplace=True)\n    nasdaq_c = nasdaq_c[\"2012-05-18\":]\n    return nasdaq_c\n\n############## REINDEX FUNCTION AND PREPARE_STOCK FUNCTION ARE PRETTY MUCH SAME, HOWEVER, I PREFER THE PRIOR ##################\n# for ARIMA or some shit    \ndef reindex(df):\n    return df.reindex(pd.date_range(df.index[0], df.index[-1])).fillna(method=\"ffill\")\n\n# for prepare_stock\ndef date_range_df(start, end, column_name = \"Time\"):\n    date_range = pd.date_range(start, end)\n    df = pd.DataFrame(date_range, columns = [column_name])\n    df.set_index(column_name, inplace=True)\n    return df\n\n# merging with date range df\ndef prepare_stock(nasdaq, start, end, stock_name=\"AAPL\", drop=True):\n    nasdaq = nasdaq.loc[nasdaq[\"Name\"]==stock_name]\n    dates = date_range_df(start, end)\n    new_nasdaq = dates.merge(nasdaq, how=\"left\", left_index=True, right_index=True)\n    if drop:\n        new_nasdaq.dropna(inplace=True)\n    return new_nasdaq\n#############################################################################################################################\n\n# create features volatility, volume, adj close\ndef get_features(nasdaq):\n    #rename Adj Close\n    nasdaq.rename(columns={\"Adj Close\":\"Adj_Close\"}, inplace=True)\n    nasdaq[\"log_Volatility\"] = np.log(nasdaq.High - nasdaq.Low + 1)\n    nasdaq[\"log_Volume\"] = np.log(nasdaq.Volume + 1) \n    nasdaq[\"log_Adj_Close\"] = np.log(nasdaq[\"Adj_Close\"] + 1)\n    # nasdaq[\"log_Adj_Close_diff\"] = nasdaq[\"log_Adj_Close\"].diff()\n    nasdaq.drop(columns = [\"Low\", \"High\", \"Close\", \"Open\", \"Name\", \"Volume\"], inplace=True)\n    # nasdaq.dropna(inplace = True)\n    return nasdaq\n\n# this will return feature engineered stock dataframe\ndef get_stock(nasdaq, stock_name=\"AAPL\"):\n    nasdaq_c = date_index_nasdaq(nasdaq)\n    stock = prepare_stock(nasdaq_c, nasdaq_c.index[0], nasdaq_c.index[-1], stock_name)\n    stock = get_features(stock)\n    stock.fillna(\"ffill\", inplace=True)\n    return stock\n\n# plot heatmap for top stocks\ndef plot_attribute(nasdaq, using,feature=\"log_Adj_Close\"):\n    stocks = pd.DataFrame()\n    for name in using:\n        stocks[name] = get_stock(nasdaq, name)[feature]\n    stocks.dropna(inplace=True)\n    stocks.plot()\n    plt.show()\n\n####### In the 2 functions below, we are adding weekday however ###########\n####### prob we could have done this in like get_stock or something #######\n# the main difference between the two is , the prior is just adding weekday at the end,\n# whereas the latter function is adding it to every stock\ndef get_train_df(nasdaq, using, features):\n    df_features_arr = reindex(get_stock(nasdaq, using[0])).to_numpy().T\n    for name in using[1:]:\n        adding = reindex(get_stock(nasdaq, name)).to_numpy().T\n        df_features_arr = np.concatenate([df_features_arr, adding])\n    df_features_arr = df_features_arr.T\n\n    ## df_features = pd.DataFrame(data=df_features_arr, columns=pd.MultiIndex.from_tuples(zip(col_one, col_two)))\n    \n    # making columns\n    # features must not include weekday here\n    if \"weekday\" in features:\n        features.remove(\"weekday\")\n    col_one = []\n    for element in using:\n        for i in range(len(features)):\n            col_one.append(element)\n    col_two = list(features)*len(using)\n    print(len(col_one), len(col_two))\n    # scaling \n    scaler = MinMaxScaler((-1, 1))\n    scaled = scaler.fit_transform(df_features_arr)\n    df_features = pd.DataFrame(data=scaled, columns=pd.MultiIndex.from_tuples(zip(col_one, col_two)))\n\n    df_features.index = pd.date_range(\"2012-05-18\", \"2021-09-10\")\n\n    day_of_week = np.array(list(map(lambda date: date.weekday(), df_features.index)))\n    day_of_week = day_of_week.reshape(-1, 1)\n    day_of_week = pd.Series(data=scaler.fit_transform(day_of_week).reshape(-1,), index = df_features.index)\n    df_features[\"weekday\"] = day_of_week\n    if \"weekday\" not in features:\n        features.append(\"weekday\")\n\n    return df_features, features\n\n\n# for feeding into network\ndef get_train_arr(nasdaq, using, features):\n    df_features_arr = []\n    for name in using:\n        arr = reindex(get_stock(nasdaq, name)).to_numpy()\n        # scaling for each column, for each stock_df in nasdaq\n        scaler = MinMaxScaler(feature_range=(-1, 1))\n        arr_scaled = scaler.fit_transform(arr)    \n\n        # adding day of week\n        day_of_week = np.array(list(map(lambda date: date.weekday(), pd.date_range(\"2012-05-18\", \"2021-09-10\"))))\n        day_of_week = day_of_week.reshape(-1, 1)\n        day_of_week = scaler.fit_transform(day_of_week)\n      \n        arr_scaled = np.concatenate([arr_scaled, day_of_week], axis=1)\n\n        df_features_arr.append(arr_scaled)\n\n\n    df_features_arr = np.array(df_features_arr)\n    if \"weekday\" not in features:\n        features.append(\"weekday\")\n    df_features_arr = df_features_arr.reshape(-1, len(features), 7)\n\n    return df_features_arr, features\n\n\ndef sliding_windows_mutli_features(data, seq_length, target_cols_ids):\n    x = []\n    y = []\n\n    for i in range((data.shape[0])-seq_length-1):\n        #change here after finishing feature engineering process\n        _x = data[i:(i+seq_length), :] \n        _y = data[i+seq_length, target_cols_ids] ## column 1 contains the labbel(log_Adj_Close)\n        x.append(_x)\n        y.append(_y)\n\n    return np.array(x), np.array(y)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-21T01:49:20.513228Z","iopub.execute_input":"2022-01-21T01:49:20.513705Z","iopub.status.idle":"2022-01-21T01:49:20.540637Z","shell.execute_reply.started":"2022-01-21T01:49:20.513666Z","shell.execute_reply":"2022-01-21T01:49:20.539911Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#################### LOAD DATA ######################\n\nnasdaq = pd.read_csv(data_root + \"NASDAQ_100_Data_From_2010.csv\", sep=\"\\t\")\n\nfeatures = ['Adj_Close', 'log_Volatility', 'log_Volume', 'log_Adj_Close']\nusing = ['FB', 'TSLA', 'AAPL', 'AMZN', 'NVDA', 'MSFT', 'GOOGL']\n# AAPL(Apple), MSFT(Microsoft), GOOGL(Google), AMZN(Amazon), TSLA(Tesla), FB(Facebook), NVDA(Nvidia)","metadata":{"execution":{"iopub.status.busy":"2022-01-21T01:49:20.541583Z","iopub.execute_input":"2022-01-21T01:49:20.541766Z","iopub.status.idle":"2022-01-21T01:49:21.166751Z","shell.execute_reply.started":"2022-01-21T01:49:20.541743Z","shell.execute_reply":"2022-01-21T01:49:21.166020Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# this part is sketchy\ndf, features = get_train_df(nasdaq, using, features)\n\nadj_close_cols_ids = []\nlog_adj_close_cols_ids = []\nvolatility_cols_ids = []\nvolume_cols_ids = []\nweekday_col_id = []\ncount = 0\nfor col in df.columns:\n    if col[1] == \"Adj_Close\":\n        df.drop(col, axis=1, inplace=True)\n        count -= 1\n    if col[1] == \"log_Adj_Close\":\n        adj_close_cols_ids.append(count)\n    if col[1] == \"log_Volume\":\n        volume_cols_ids.append(count)\n    if col[1] == \"log_Volatility\":\n        volatility_cols_ids.append(count)\n    if col[0] == \"weekday\":\n        weekday_col_id.append(count)\n    count += 1\ndf = df.to_numpy()\nx, y = sliding_windows_mutli_features(df, 30, adj_close_cols_ids)\n\nx.shape, y.shape\n","metadata":{"execution":{"iopub.status.busy":"2022-01-21T01:49:21.168830Z","iopub.execute_input":"2022-01-21T01:49:21.169055Z","iopub.status.idle":"2022-01-21T01:49:22.207038Z","shell.execute_reply.started":"2022-01-21T01:49:21.169027Z","shell.execute_reply":"2022-01-21T01:49:22.206316Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# train test split (70:30)\ntrain_size = int(len(y)*0.80)\ntest_size = len(y) - train_size\n\ndataX = Variable(torch.Tensor(np.array(x)))\ndataY = Variable(torch.Tensor(np.array(y)))\n\ntrainX = Variable(torch.Tensor(np.array(x[0:train_size])))\ntrainY = Variable(torch.Tensor(np.array(y[0:train_size])))\n\ntestX = Variable(torch.Tensor(np.array(x[train_size:len(x)])))\ntestY = Variable(torch.Tensor(np.array(y[train_size:len(y)])))\n\nprint(\"train shape is:\",trainX.size())\nprint(\"train label shape is:\",trainY.size())\nprint(\"test shape is:\",testX.size())\nprint(\"test label shape is:\",testY.size())","metadata":{"execution":{"iopub.status.busy":"2022-01-21T01:49:22.208215Z","iopub.execute_input":"2022-01-21T01:49:22.208660Z","iopub.status.idle":"2022-01-21T01:49:22.250458Z","shell.execute_reply.started":"2022-01-21T01:49:22.208620Z","shell.execute_reply":"2022-01-21T01:49:22.249665Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2022-01-21T01:49:22.252623Z","iopub.execute_input":"2022-01-21T01:49:22.253114Z","iopub.status.idle":"2022-01-21T01:49:22.965064Z","shell.execute_reply.started":"2022-01-21T01:49:22.253076Z","shell.execute_reply":"2022-01-21T01:49:22.964259Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda')\n\nclass LSTM2(nn.Module):\n\n    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n        super(LSTM2, self).__init__()\n        \n        self.num_classes = num_classes\n        self.num_layers = num_layers\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        #self.seq_length = seq_length\n\n        \n        # what does the batch_first do\n        self.LSTM2 = nn.LSTM(\\\n            input_size=input_size, \n            hidden_size=hidden_size,\n            num_layers=num_layers, \n            batch_first=True,\n            dropout = 0.25)\n        \n        # Linear(in_features, out_features)\n        self.fc1 = nn.Linear(hidden_size, 256)                                                                                                                                                                                                                           \n        self.bn1 = nn.BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        self.dp1 = nn.Dropout(0.25)\n\n        self.fc2 = nn.Linear(256, 128)\n        self.bn2 = nn.BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        self.dp2 = nn.Dropout(0.2)\n\n        self.fc3 = nn.Linear(128, 7)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        h_1 = Variable(torch.zeros(\n            self.num_layers, x.size(0), self.hidden_size).to(device))\n        \n        c_1 = Variable(torch.zeros(\n            self.num_layers, x.size(0), self.hidden_size).to(device))\n        \n        # Propagate input through LSTM\n        _, (hn, cn) = self.LSTM2(x, (h_1, c_1))\n        y = hn.view(-1, self.hidden_size)\n\n        final_state = hn.view(self.num_layers, x.size(0), self.hidden_size)[-1]\n\n        x0 = self.fc1(final_state)\n        x0 = self.bn1(x0)\n        x0 = self.dp1(x0)\n        x0 = self.relu(x0)\n\n        x0 = self.fc2(x0)\n        x0 = self.bn2(x0)\n        x0 = self.dp2(x0)\n\n        x0 = self.relu(x0)\n        \n        out = self.fc3(x0)                                         \n        # out = self.dropout(out)\n       \n        return out\n\ndef init_weights(model):\n    for name, param in model.named_parameters():\n        nn.init.uniform_(param.data, -0.88, 0.08)\n\n# create a nn class (just-for-fun choice :-) \nclass RMSELoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.mse = nn.MSELoss()\n        \n    def forward(self,yhat,y):\n        return torch.sqrt(self.mse(yhat,y))","metadata":{"execution":{"iopub.status.busy":"2022-01-21T01:49:22.966646Z","iopub.execute_input":"2022-01-21T01:49:22.966868Z","iopub.status.idle":"2022-01-21T01:49:22.984644Z","shell.execute_reply.started":"2022-01-21T01:49:22.966839Z","shell.execute_reply":"2022-01-21T01:49:22.983728Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"###### Parameters #######\nnum_epochs = 700\nlearning_rate = 1e-3\ninput_size = 22 # feature nums\nhidden_size = 512\nnum_layers = 2\nnum_classes = 7 # because we are using 7 stocks\n#########################","metadata":{"execution":{"iopub.status.busy":"2022-01-21T01:49:22.985761Z","iopub.execute_input":"2022-01-21T01:49:22.986065Z","iopub.status.idle":"2022-01-21T01:49:22.996498Z","shell.execute_reply.started":"2022-01-21T01:49:22.986029Z","shell.execute_reply":"2022-01-21T01:49:22.995793Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"trainX.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-21T01:49:22.999499Z","iopub.execute_input":"2022-01-21T01:49:22.999713Z","iopub.status.idle":"2022-01-21T01:49:23.008334Z","shell.execute_reply.started":"2022-01-21T01:49:22.999682Z","shell.execute_reply":"2022-01-21T01:49:23.007525Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"############################################################################################\n############################################################################################\n############################### ONLY RUN FOR TRAINING ######################################\n############################################################################################\n############################################################################################\nbest_val_loss = 100 \n### Init Model\nlstm = LSTM2(num_classes, input_size, hidden_size, num_layers)\nlstm.to(device)\nlstm.apply(init_weights)\n\n### Set Criterion Optimizer and scheduler\ncriterion = torch.nn.MSELoss().to(device) \noptimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate, weight_decay=1e-5)\n\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=100, factor=0.5, min_lr=1e-7, eps=1e-08)\n\n#optimizer = torch.optim.SGD(lstm.parameters(), lr=learning_rate)\n\n# Train model\nfor epoch in progress_bar(range(num_epochs)):\n    lstm.train()\n    outputs= lstm(trainX.to(device))\n    optimizer.zero_grad()\n    torch.nn.utils.clip_grad_norm_(lstm.parameters(),1)\n\n    # obtain loss func\n    loss = criterion(outputs, trainY.to(device))\n    loss.backward()\n\n    scheduler.step(loss)\n    optimizer.step()\n\n    #evaluate on test\n    lstm.eval()\n    valid = lstm(testX.to(device))\n    vall_loss = criterion(valid, testY.to(device))\n\n    scheduler.step(vall_loss)\n\n    if vall_loss.cpu().item() < best_val_loss:\n        torch.save(lstm.state_dict(), 'best_model.pt')\n        print(\"saved best model epoch:\",epoch,\"val loss is:\",vall_loss.cpu().item())\n        best_val_loss = vall_loss.cpu().item()\n\n    if epoch % 50 == 0:\n        print(f\"Epoch: {epoch}, loss: {loss.cpu().item()}, valid loss:{vall_loss.cpu().item()}\")\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-21T01:49:23.009518Z","iopub.execute_input":"2022-01-21T01:49:23.009698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testX.cpu().detach().numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lstm(testX.to(device))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(5, 20))\nlstm.eval()\npred = lstm(testX[:20, :, :].to(device)).cpu().detach().numpy()\nplt.plot(testY[:20, :].cpu(), c=\"blue\")\nplt.plot(pred, c=\"red\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testX_c = testX.cpu().detach().numpy().copy()\n\n# testX_drop_Volatility = np.delete(testX_c, [0, 3, 6, 9, 12, 15, 18], 2)\n# testX_drop_Volume = np.delete(testX_c, [1, 4, 7, 10, 13, 16, 19], 2)\n# testX_drop_Adj_Close = np.delete(testX_c, target_cols_ids, 2)\ntestX_drop_Volatility = testX_c.copy()\ntestX_drop_Volatility[:,:,volatility_cols_ids] = 0.\ntestX_drop_Volume = testX_c.copy()\ntestX_drop_Volume[:,:,volume_cols_ids] = 0.\ntestX_drop_Adj_Close = testX_c.copy()\ntestX_drop_Adj_Close[:, :, adj_close_cols_ids] = 0.\n\ntestX_drop_Volatility = Variable(torch.Tensor(testX_drop_Volatility))\ntestX_drop_Volume = Variable(torch.Tensor(testX_drop_Volume))\ntestX_drop_Adj_Close = Variable(torch.Tensor(testX_drop_Adj_Close))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}