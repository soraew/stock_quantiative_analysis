{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "below are imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"../../data/\"\n",
    "#stats stuff\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "\n",
    "# ML stuff\n",
    "import numpy as np\n",
    "from numpy.fft import *\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Lasso\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "# DL stuff\n",
    "from torch.autograd import Variable\n",
    "from fastprogress import master_bar, progress_bar\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "# basic stuff\n",
    "import datetime\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# relative imports\n",
    "# from LSTM_preprocess import get_Xy, get_train_test, get_train_df, sliding_windows_mutli_features\n",
    "# from hfuncs import check_mkdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "below are funcitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "################## this cell is dedicated to kaggle ####################\n",
    "########################################################################\n",
    "\n",
    "# set index as datetime\n",
    "def date_index_nasdaq(nasdaq):\n",
    "    nasdaq_c = nasdaq.copy()\n",
    "    dates = pd.to_datetime(nasdaq_c.Date)\n",
    "    nasdaq_c.set_index(dates, inplace=True)\n",
    "    # set date as index\n",
    "    nasdaq_c.drop(\"Date\", axis=1, inplace=True)\n",
    "    # ここでFBとかTESLAとかに合わせている\n",
    "    nasdaq_c = nasdaq_c[\"2012-05-18\":]\n",
    "    return nasdaq_c\n",
    "\n",
    "############## REINDEX FUNCTION AND PREPARE_STOCK FUNCTION ARE PRETTY MUCH SAME, HOWEVER, I PREFER THE PRIOR ##################\n",
    "# for ARIMA or some shit    \n",
    "def reindex(df):\n",
    "    return df.reindex(pd.date_range(df.index[0], df.index[-1])).fillna(method=\"ffill\")\n",
    "\n",
    "# for prepare_stock\n",
    "def date_range_df(start, end, column_name = \"Time\"):\n",
    "    date_range = pd.date_range(start, end)\n",
    "    df = pd.DataFrame(date_range, columns = [column_name])\n",
    "    df.set_index(column_name, inplace=True)\n",
    "    return df\n",
    "\n",
    "# merging with date range df\n",
    "def prepare_stock(nasdaq, start, end, stock_name=\"AAPL\", drop=True):\n",
    "    nasdaq = nasdaq.loc[nasdaq[\"Name\"]==stock_name]\n",
    "    dates = date_range_df(start, end)\n",
    "    new_nasdaq = dates.merge(nasdaq, how=\"left\", left_index=True, right_index=True)\n",
    "    if drop:\n",
    "        new_nasdaq.dropna(inplace=True)\n",
    "    return new_nasdaq\n",
    "#############################################################################################################################\n",
    "\n",
    "# create features volatility, volume, adj close\n",
    "# def get_features(nasdaq, features):\n",
    "#     #rename Adj Close\n",
    "#     nasdaq.rename(columns={\"Adj Close\":\"Adj_Close\"}, inplace=True)\n",
    "#     nasdaq[\"log_Volatility\"] = np.log(nasdaq.High - nasdaq.Low + 1)\n",
    "#     nasdaq[\"log_Volume\"] = np.log(nasdaq.Volume + 1) \n",
    "#     nasdaq[\"log_Adj_Close\"] = np.log(nasdaq[\"Adj_Close\"] + 1)\n",
    "#     # nasdaq[\"log_Adj_Close_diff\"] = nasdaq[\"log_Adj_Close\"].diff()\n",
    "    \n",
    "#     nasdaq.drop(columns = [\"Low\", \"High\", \"Close\", \"Open\", \"Name\", \"Volume\"], inplace=True)\n",
    "#     # nasdaq.dropna(inplace = True)\n",
    "#     return nasdaq\n",
    "\n",
    "def get_features(df, features):\n",
    "    #rename Adj Close\n",
    "    \n",
    "    df.rename(columns={\"Adj Close\":\"Adj_Close\"}, inplace=True) \n",
    "    df[\"log_Volatility\"] = np.log(df.High - df.Low + 1)\n",
    "    df[\"log_Volume\"] = np.log(df.Volume + 1) \n",
    "    df[\"log_Adj_Close\"] = np.log(df[\"Adj_Close\"] + 1)\n",
    "    # df[\"day_of_week\"] = np.array(list(map(lambda date: date.weekday(), df.index)))\n",
    "\n",
    "    if 'Adj_Close' not in features:\n",
    "        df.drop(columns=[\"Adj_Close\"], inplace=True)\n",
    "    # nasdaq[\"log_Adj_Close_diff\"] = nasdaq[\"log_Adj_Close\"].diff()\n",
    "\n",
    "    df.drop(columns = [\"Low\", \"High\", \"Close\", \"Open\", \"Name\", \"Volume\"], inplace=True)\n",
    "    # nasdaq = nasdaq[features]\n",
    "\n",
    "    # nasdaq.dropna(inplace = True)\n",
    "    return df\n",
    "\n",
    "# this will return feature engineered stock dataframe\n",
    "def get_stock(nasdaq, features, stock_name=\"AAPL\"):\n",
    "    nasdaq_c = date_index_nasdaq(nasdaq)\n",
    "    stock = prepare_stock(nasdaq_c, nasdaq_c.index[0], nasdaq_c.index[-1], stock_name)\n",
    "    stock = get_features(stock, features)\n",
    "    stock.fillna(\"ffill\", inplace=True)\n",
    "    return stock\n",
    "\n",
    "# plot heatmap for top stocks\n",
    "def plot_attribute(nasdaq, using,feature=\"log_Adj_Close\"):\n",
    "    stocks = pd.DataFrame()\n",
    "    for name in using:\n",
    "        stocks[name] = get_stock(nasdaq, name)[feature]\n",
    "    stocks.dropna(inplace=True)\n",
    "    stocks.plot()\n",
    "    plt.show()\n",
    "\n",
    "####### In the 2 functions below, we are adding weekday however ###########\n",
    "####### prob we could have done this in like get_stock or something #######\n",
    "# the main difference between the two is , the prior is just adding weekday at the end,\n",
    "# whereas the latter function is adding it to every stock\n",
    "def get_train_df(nasdaq, using, features):\n",
    "    df_features_arr = reindex(get_stock(nasdaq, using[0])).to_numpy().T\n",
    "    for name in using[1:]:\n",
    "        adding = reindex(get_stock(nasdaq, name)).to_numpy().T\n",
    "        df_features_arr = np.concatenate([df_features_arr, adding])\n",
    "    df_features_arr = df_features_arr.T\n",
    "\n",
    "    ## df_features = pd.DataFrame(data=df_features_arr, columns=pd.MultiIndex.from_tuples(zip(col_one, col_two)))\n",
    "    \n",
    "    # making columns\n",
    "    # features must not include weekday here\n",
    "    if \"weekday\" in features:\n",
    "        features.remove(\"weekday\")\n",
    "    col_one = []\n",
    "    for element in using:\n",
    "        for i in range(len(features)):\n",
    "            col_one.append(element)\n",
    "    col_two = list(features)*len(using)\n",
    "    # print(len(col_one), len(col_two))\n",
    "\n",
    "    # scaling \n",
    "    scaler = MinMaxScaler((-1, 1))\n",
    "    scaled = scaler.fit_transform(df_features_arr)\n",
    "    df_features = pd.DataFrame(data=scaled, columns=pd.MultiIndex.from_tuples(zip(col_one, col_two)))\n",
    "\n",
    "    df_features.index = pd.date_range(\"2012-05-18\", \"2021-09-10\")\n",
    "\n",
    "    day_of_week = np.array(list(map(lambda date: date.weekday(), df_features.index)))\n",
    "    day_of_week = day_of_week.reshape(-1, 1)\n",
    "    day_of_week = pd.Series(data=scaler.fit_transform(day_of_week).reshape(-1,), index = df_features.index)\n",
    "    df_features[\"weekday\"] = day_of_week\n",
    "    if \"weekday\" not in features:\n",
    "        features.append(\"weekday\")\n",
    "\n",
    "    return df_features, features\n",
    "\n",
    "\n",
    "# for feeding into network\n",
    "def get_train_arr(nasdaq, using, features):\n",
    "    df_features_arr = []\n",
    "    for name in using:\n",
    "        arr = reindex(get_stock(nasdaq, name)).to_numpy()\n",
    "        # scaling for each column, for each stock_df in nasdaq\n",
    "        scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "        arr_scaled = scaler.fit_transform(arr)    \n",
    "\n",
    "        # adding day of week\n",
    "        day_of_week = np.array(list(map(lambda date: date.weekday(), pd.date_range(\"2012-05-18\", \"2021-09-10\"))))\n",
    "        day_of_week = day_of_week.reshape(-1, 1)\n",
    "        day_of_week = scaler.fit_transform(day_of_week)\n",
    "      \n",
    "        arr_scaled = np.concatenate([arr_scaled, day_of_week], axis=1)\n",
    "\n",
    "        df_features_arr.append(arr_scaled)\n",
    "\n",
    "\n",
    "    df_features_arr = np.array(df_features_arr)\n",
    "    if \"weekday\" not in features:\n",
    "        features.append(\"weekday\")\n",
    "    df_features_arr = df_features_arr.reshape(-1, len(features), 7)\n",
    "\n",
    "    return df_features_arr, features\n",
    "\n",
    "\n",
    "def sliding_windows_mutli_features(data, seq_length, target_cols_ids):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for i in range((data.shape[0])-seq_length-1):\n",
    "        #change here after finishing feature engineering process\n",
    "        _x = data[i:(i+seq_length), :] \n",
    "        _y = data[i+seq_length, target_cols_ids] ## column 1 contains the labbel(log_Adj_Close)\n",
    "        x.append(_x)\n",
    "        y.append(_y)\n",
    "\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "def get_Xy(df, window_size):\n",
    "    log_adj_close_cols_ids = []\n",
    "    volatility_cols_ids = []\n",
    "    volume_cols_ids = []\n",
    "    weekday_col_id = []\n",
    "    count = 0\n",
    "    for col in df.columns:\n",
    "        # print(col)\n",
    "        if col[1] == \"Adj_Close\":\n",
    "            df.drop(col, axis=1, inplace=True)\n",
    "            count -= 1\n",
    "        if col[1] == \"log_Adj_Close\":\n",
    "            log_adj_close_cols_ids.append(count)\n",
    "        if col[1] == \"log_Volume\":\n",
    "            volume_cols_ids.append(count)\n",
    "        if col[1] == \"log_Volatility\":\n",
    "            volatility_cols_ids.append(count)\n",
    "        if col[0] == \"weekday\":\n",
    "            weekday_col_id.append(count)\n",
    "        count += 1\n",
    "    df = df.to_numpy()\n",
    "    x, y = sliding_windows_mutli_features(df, window_size, log_adj_close_cols_ids)\n",
    "\n",
    "    # x.shape, y.shape\n",
    "    return x, y\n",
    "\n",
    "def get_train_test(x, y, train_ratio):\n",
    "    train_size = int(len(y)*train_ratio)\n",
    "    test_size = len(y) - train_size\n",
    "\n",
    "    dataX = Variable(torch.Tensor(np.array(x)))\n",
    "    dataY = Variable(torch.Tensor(np.array(y)))\n",
    "\n",
    "    trainX = Variable(torch.Tensor(np.array(x[0:train_size])))\n",
    "    trainY = Variable(torch.Tensor(np.array(y[0:train_size])))\n",
    "\n",
    "    testX = Variable(torch.Tensor(np.array(x[train_size:len(x)])))\n",
    "    testY = Variable(torch.Tensor(np.array(y[train_size:len(y)])))\n",
    "\n",
    "    return trainX, trainY, testX, testY\n",
    "\n",
    "\n",
    "def check_mkdir(dir_name):\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.mkdir(dir_name)\n",
    "\n",
    "def random_search_lgbm(X, y, params_space):\n",
    "    params_log = {}\n",
    "    iteration = 40\n",
    "    cv = TimeSeriesSplit()\n",
    "    for i in tqdm(range(iteration)):\n",
    "        params = {}\n",
    "        for key in params_space.keys():\n",
    "            param_list = params_space[key]\n",
    "            length = len(param_list)\n",
    "            idx =np.random.randint(0,length) \n",
    "            params.update({key:param_list[idx]})\n",
    "            # fit model to data\n",
    "        \n",
    "        model = lgb.LGBMRegressor(**params)\n",
    "        for train_idx, test_idx, in cv.split(X):\n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test).squeeze()\n",
    "            coef_score = np.corrcoef(y_pred, y_test)[0][1]\n",
    "            rmse_score = np.sqrt(mean_squared_error(y_pred, y_test))\n",
    "        params_log.update({i:[coef_score, rmse_score, params]})\n",
    "\n",
    "    sorted_by_coef=sorted(params_log.items(), key = lambda item: item[1][0], reverse=True)\n",
    "    sorted_by_rmse=sorted(params_log.items(), key = lambda item: item[1][1])\n",
    "    \n",
    "    return sorted_by_coef, sorted_by_rmse\n",
    "\n",
    "\n",
    "def binary_y(y_np, criterion):\n",
    "    for i in range(len(y_np)):\n",
    "        if y_np[i] > criterion:\n",
    "            y_np[i] = int(1)\n",
    "        else:\n",
    "            y_np[i] = int(0)\n",
    "        \n",
    "    return y_np\n",
    "\n",
    "\n",
    "def sliding_windows_single_feature(X, y, seq_length):\n",
    "    x = []\n",
    "    Y = []\n",
    "\n",
    "    for i in range((X.shape[0])-seq_length-1):\n",
    "        #change here after finishing feature engineering process\n",
    "        _x = X[i:(i+seq_length), :] \n",
    "        _y = y[i+seq_length] ## column 1 contains the labbel(log_Adj_Close)\n",
    "        x.append(_x)\n",
    "        Y.append(_y)\n",
    "\n",
    "    return np.array(x), np.array(Y)\n",
    "\n",
    "def get_bc_per_stock_Xy(nasdaq, features, stock_name):\n",
    "    stock = get_stock(nasdaq, features, stock_name)\n",
    "\n",
    "    stock_new = stock.copy()\n",
    "    stock_log_adj = stock[\"log_Adj_Close\"]\n",
    "    stock_log_adj_diff = stock_log_adj.shift(-1) - stock_log_adj\n",
    "    stock_new[\"log_Adj_Close\"] = stock_log_adj_diff # tomorrow - today\n",
    "    X = stock_new\n",
    "    y = stock_new[\"log_Adj_Close\"].iloc[1:-1] #　そのまま, getting rid of nan although ind 0 is not nan for y\n",
    "    X[\"log_Adj_Close\"] = stock_new[\"log_Adj_Close\"].shift(1) # today - yesterday\n",
    "    X = X.iloc[1:-1] # getting rid of nans\n",
    "    new_index = y.index\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    X = scaler.fit_transform(X)\n",
    "    y = scaler.fit_transform(y.to_numpy().reshape(-1,1))\n",
    "\n",
    "    # to 1 and 0 two class classification\n",
    "    X[:, 2] = binary_y(X[:, 2], c)\n",
    "    y = binary_y(y, c)\n",
    "\n",
    "    X, y = sliding_windows_single_feature(X, y, 50)\n",
    "\n",
    "    train_size = int(X.shape[0]*train_ratio)\n",
    "    X_train, X_test = X[:train_size, :, :], X[train_size:, :, :]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "below loads data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### LOAD DATA ######################\n",
    "################ DEFINE CONSTANTS ###################\n",
    "##### we will only predict for one stock here #######\n",
    "\n",
    "nasdaq = pd.read_csv(data_root + \"NASDAQ_100_Data_From_2010.csv\", sep=\"\\t\")\n",
    "\n",
    "c = 0.14 # this is for scaled apple stock so that the two classes have roughly the same amount of data points\n",
    "window_size = 50\n",
    "train_ratio = 0.80\n",
    "stock_name = \"AAPL\"\n",
    "DROPOUT = 0.2\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "# the order of the features is extremely important for latter indexing so make sure\n",
    "features = ['log_Volatility', 'log_Volume', 'log_Adj_Close']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_bc_per_stock_Xy(nasdaq, features, stock_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_4 = torch.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./']\n",
      "['./.ipynb_checkpoints']\n"
     ]
    }
   ],
   "source": [
    "best_model_int = lambda string: int(re.split(r\"best_model_([0-9]*).pt\", string)[1])\n",
    "dirname_idx = lambda string: re.split(r\"bc_([0-9]*)\", string)\n",
    "\n",
    "all_files = {}\n",
    "for dirname, _, filenames in os.walk(\"./\"):\n",
    "    dirname_splitted = dirname_idx(dirname)\n",
    "    if len(dirname_splitted) == 3:\n",
    "        files= []\n",
    "        for filename in filenames:\n",
    "            files.append(os.path.join(dirname, filename))\n",
    "        # all_files[f\"\"]\n",
    "        all_files[f\"{dirname_splitted[1]}\"] = files\n",
    "    else:\n",
    "        print(dirname_splitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'64': ['./AAPL_bc_64/best_model_7.pt',\n  './AAPL_bc_64/preds.pt',\n  './AAPL_bc_64/best_model_2.pt',\n  './AAPL_bc_64/best_model_34.pt',\n  './AAPL_bc_64/losses.pt',\n  './AAPL_bc_64/best_model_31.pt',\n  './AAPL_bc_64/best_model_35.pt',\n  './AAPL_bc_64/best_model_8.pt',\n  './AAPL_bc_64/best_model_16.pt',\n  './AAPL_bc_64/best_model_32.pt',\n  './AAPL_bc_64/best_model_36.pt',\n  './AAPL_bc_64/best_model_37.pt',\n  './AAPL_bc_64/best_model_33.pt',\n  './AAPL_bc_64/best_model_38.pt',\n  './AAPL_bc_64/best_model_1.pt',\n  './AAPL_bc_64/best_model_0.pt'],\n '4': ['./AAPL_bc_4/preds.pt',\n  './AAPL_bc_4/losses.pt',\n  './AAPL_bc_4/best_model_0.pt'],\n '512': ['./AAPL_bc_512/best_model_51.pt',\n  './AAPL_bc_512/best_model_87.pt',\n  './AAPL_bc_512/best_model_97.pt',\n  './AAPL_bc_512/best_model_8.pt',\n  './AAPL_bc_512/best_model_80.pt',\n  './AAPL_bc_512/best_model_47.pt',\n  './AAPL_bc_512/best_model_53.pt',\n  './AAPL_bc_512/best_model_72.pt',\n  './AAPL_bc_512/best_model_33.pt',\n  './AAPL_bc_512/best_model_5.pt',\n  './AAPL_bc_512/best_model_1.pt',\n  './AAPL_bc_512/best_model_107.pt',\n  './AAPL_bc_512/best_model_0.pt'],\n '32': ['./AAPL_bc_32/best_model_3.pt',\n  './AAPL_bc_32/preds.pt',\n  './AAPL_bc_32/best_model_2.pt',\n  './AAPL_bc_32/best_model_10.pt',\n  './AAPL_bc_32/losses.pt',\n  './AAPL_bc_32/best_model_51.pt',\n  './AAPL_bc_32/best_model_45.pt',\n  './AAPL_bc_32/best_model_55.pt',\n  './AAPL_bc_32/best_model_21.pt',\n  './AAPL_bc_32/best_model_44.pt',\n  './AAPL_bc_32/best_model_54.pt',\n  './AAPL_bc_32/best_model_50.pt',\n  './AAPL_bc_32/best_model_22.pt',\n  './AAPL_bc_32/best_model_47.pt',\n  './AAPL_bc_32/best_model_43.pt',\n  './AAPL_bc_32/best_model_53.pt',\n  './AAPL_bc_32/best_model_52.pt',\n  './AAPL_bc_32/best_model_23.pt',\n  './AAPL_bc_32/best_model_46.pt',\n  './AAPL_bc_32/best_model_56.pt',\n  './AAPL_bc_32/best_model_49.pt',\n  './AAPL_bc_32/best_model_1.pt',\n  './AAPL_bc_32/best_model_48.pt',\n  './AAPL_bc_32/best_model_0.pt',\n  './AAPL_bc_32/best_model_4.pt'],\n '256': ['./AAPL_bc_256/best_model_7.pt',\n  './AAPL_bc_256/best_model_98.pt',\n  './AAPL_bc_256/preds.pt',\n  './AAPL_bc_256/best_model_101.pt',\n  './AAPL_bc_256/best_model_99.pt',\n  './AAPL_bc_256/best_model_100.pt',\n  './AAPL_bc_256/best_model_2.pt',\n  './AAPL_bc_256/best_model_96.pt',\n  './AAPL_bc_256/best_model_34.pt',\n  './AAPL_bc_256/losses.pt',\n  './AAPL_bc_256/best_model_11.pt',\n  './AAPL_bc_256/best_model_97.pt',\n  './AAPL_bc_256/best_model_95.pt',\n  './AAPL_bc_256/best_model_33.pt',\n  './AAPL_bc_256/best_model_103.pt',\n  './AAPL_bc_256/best_model_49.pt',\n  './AAPL_bc_256/best_model_0.pt',\n  './AAPL_bc_256/best_model_102.pt'],\n '16': ['./AAPL_bc_16/best_model_3.pt',\n  './AAPL_bc_16/best_model_7.pt',\n  './AAPL_bc_16/preds.pt',\n  './AAPL_bc_16/best_model_6.pt',\n  './AAPL_bc_16/best_model_2.pt',\n  './AAPL_bc_16/best_model_10.pt',\n  './AAPL_bc_16/best_model_34.pt',\n  './AAPL_bc_16/losses.pt',\n  './AAPL_bc_16/best_model_51.pt',\n  './AAPL_bc_16/best_model_9.pt',\n  './AAPL_bc_16/best_model_14.pt',\n  './AAPL_bc_16/best_model_30.pt',\n  './AAPL_bc_16/best_model_15.pt',\n  './AAPL_bc_16/best_model_31.pt',\n  './AAPL_bc_16/best_model_11.pt',\n  './AAPL_bc_16/best_model_35.pt',\n  './AAPL_bc_16/best_model_50.pt',\n  './AAPL_bc_16/best_model_8.pt',\n  './AAPL_bc_16/best_model_16.pt',\n  './AAPL_bc_16/best_model_32.pt',\n  './AAPL_bc_16/best_model_12.pt',\n  './AAPL_bc_16/best_model_36.pt',\n  './AAPL_bc_16/best_model_13.pt',\n  './AAPL_bc_16/best_model_17.pt',\n  './AAPL_bc_16/best_model_33.pt',\n  './AAPL_bc_16/best_model_5.pt',\n  './AAPL_bc_16/best_model_49.pt',\n  './AAPL_bc_16/best_model_18.pt',\n  './AAPL_bc_16/best_model_1.pt',\n  './AAPL_bc_16/best_model_19.pt',\n  './AAPL_bc_16/best_model_0.pt',\n  './AAPL_bc_16/best_model_29.pt',\n  './AAPL_bc_16/best_model_4.pt'],\n '8': ['./AAPL_bc_8/preds.pt',\n  './AAPL_bc_8/best_model_24.pt',\n  './AAPL_bc_8/losses.pt',\n  './AAPL_bc_8/best_model_25.pt',\n  './AAPL_bc_8/best_model_26.pt',\n  './AAPL_bc_8/best_model_27.pt',\n  './AAPL_bc_8/best_model_28.pt',\n  './AAPL_bc_8/best_model_1.pt',\n  './AAPL_bc_8/best_model_0.pt'],\n '128': ['./AAPL_bc_128/best_model_3.pt',\n  './AAPL_bc_128/preds.pt',\n  './AAPL_bc_128/best_model_34.pt',\n  './AAPL_bc_128/losses.pt',\n  './AAPL_bc_128/best_model_71.pt',\n  './AAPL_bc_128/best_model_11.pt',\n  './AAPL_bc_128/best_model_25.pt',\n  './AAPL_bc_128/best_model_35.pt',\n  './AAPL_bc_128/best_model_16.pt',\n  './AAPL_bc_128/best_model_12.pt',\n  './AAPL_bc_128/best_model_26.pt',\n  './AAPL_bc_128/best_model_17.pt',\n  './AAPL_bc_128/best_model_1.pt',\n  './AAPL_bc_128/best_model_0.pt']}"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = {}\n",
    "for dir_idx in all_files.keys():\n",
    "    preds[dir_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('pyflux': conda)",
   "name": "python3812jvsc74a57bd0b8bf3bbb95d33652ea8a09e83516ae388afde8f9530fb9552010d87506ab4938"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "metadata": {
   "interpreter": {
    "hash": "b8bf3bbb95d33652ea8a09e83516ae388afde8f9530fb9552010d87506ab4938"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}