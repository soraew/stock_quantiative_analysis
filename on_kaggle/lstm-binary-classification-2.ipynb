{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"data_root = \"/kaggle/input/nasdaq100-stock-price-data/\"\n#stats stuff\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\n\n# ML stuff\nimport numpy as np\nfrom numpy.fft import *\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import Lasso\nimport pandas as pd\nimport lightgbm as lgb\n\n\n# DL stuff\nfrom torch.autograd import Variable\nfrom fastprogress import master_bar, progress_bar\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n\n# plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\n# basic stuff\nimport datetime\nimport io\nimport os\nfrom os.path import join\nfrom collections import Counter\nfrom tqdm import tqdm\n\n# relative imports\n# from LSTM_preprocess import get_Xy, get_train_test, get_train_df, sliding_windows_mutli_features\n# from hfuncs import check_mkdir","metadata":{"execution":{"iopub.status.busy":"2022-01-22T10:15:19.571242Z","iopub.execute_input":"2022-01-22T10:15:19.571565Z","iopub.status.idle":"2022-01-22T10:15:24.244265Z","shell.execute_reply.started":"2022-01-22T10:15:19.571474Z","shell.execute_reply":"2022-01-22T10:15:24.243455Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"- functions cell below","metadata":{}},{"cell_type":"code","source":"########################################################################\n################## this cell is dedicated to kaggle ####################\n########################################################################\n\n# set index as datetime\ndef date_index_nasdaq(nasdaq):\n    nasdaq_c = nasdaq.copy()\n    dates = pd.to_datetime(nasdaq_c.Date)\n    nasdaq_c.set_index(dates, inplace=True)\n    # set date as index\n    nasdaq_c.drop(\"Date\", axis=1, inplace=True)\n    # ここでFBとかTESLAとかに合わせている\n    nasdaq_c = nasdaq_c[\"2012-05-18\":]\n    return nasdaq_c\n\n############## REINDEX FUNCTION AND PREPARE_STOCK FUNCTION ARE PRETTY MUCH SAME, HOWEVER, I PREFER THE PRIOR ##################\n# for ARIMA or some shit    \ndef reindex(df):\n    return df.reindex(pd.date_range(df.index[0], df.index[-1])).fillna(method=\"ffill\")\n\n# for prepare_stock\ndef date_range_df(start, end, column_name = \"Time\"):\n    date_range = pd.date_range(start, end)\n    df = pd.DataFrame(date_range, columns = [column_name])\n    df.set_index(column_name, inplace=True)\n    return df\n\n# merging with date range df\ndef prepare_stock(nasdaq, start, end, stock_name=\"AAPL\", drop=True):\n    nasdaq = nasdaq.loc[nasdaq[\"Name\"]==stock_name]\n    dates = date_range_df(start, end)\n    new_nasdaq = dates.merge(nasdaq, how=\"left\", left_index=True, right_index=True)\n    if drop:\n        new_nasdaq.dropna(inplace=True)\n    return new_nasdaq\n#############################################################################################################################\n\n# create log_Volatility, log_Volume, log_Adj_Close and drop Adj_Close if not included in features\ndef get_features(df, features):\n    #rename Adj Close\n    \n    \n    df.rename(columns={\"Adj Close\":\"Adj_Close\"}, inplace=True) \n    df[\"log_Volatility\"] = np.log(df.High - df.Low + 1)\n    df[\"log_Volume\"] = np.log(df.Volume + 1) \n    df[\"log_Adj_Close\"] = np.log(df[\"Adj_Close\"] + 1)\n    # df[\"day_of_week\"] = np.array(list(map(lambda date: date.weekday(), df.index)))\n\n    if 'Adj_Close' not in features:\n        df.drop(columns=[\"Adj_Close\"], inplace=True)\n    # nasdaq[\"log_Adj_Close_diff\"] = nasdaq[\"log_Adj_Close\"].diff()\n\n    df.drop(columns = [\"Low\", \"High\", \"Close\", \"Open\", \"Name\", \"Volume\"], inplace=True)\n    # nasdaq = nasdaq[features]\n\n    # nasdaq.dropna(inplace = True)\n    return df\n\n# this will return feature engineered stock dataframe\ndef get_stock(nasdaq, features, stock_name=\"AAPL\"):\n    nasdaq_c = date_index_nasdaq(nasdaq)\n    stock = prepare_stock(nasdaq_c, nasdaq_c.index[0], nasdaq_c.index[-1], stock_name)\n    stock = get_features(stock, features)\n    stock.fillna(\"ffill\", inplace=True)\n    return stock\n\n# plot heatmap for top stocks\ndef plot_attribute(nasdaq, using,feature=\"log_Adj_Close\"):\n    stocks = pd.DataFrame()\n    for name in using:\n        stocks[name] = get_stock(nasdaq, name)[feature]\n    stocks.dropna(inplace=True)\n    stocks.plot()\n    plt.show()\n\n\n\n\ndef sliding_windows_mutli_features(data, seq_length, target_cols_ids):\n    x = []\n    y = []\n\n    for i in range((data.shape[0])-seq_length-1):\n        #change here after finishing feature engineering process\n        _x = data[i:(i+seq_length), :] \n        _y = data[i+seq_length, target_cols_ids] ## column 1 contains the labbel(log_Adj_Close)\n        x.append(_x)\n        y.append(_y)\n\n    return np.array(x), np.array(y)\n\ndef get_Xy(df, window_size):\n    log_adj_close_cols_ids = []\n    volatility_cols_ids = []\n    volume_cols_ids = []\n    weekday_col_id = []\n    count = 0\n    for col in df.columns:\n        # print(col)\n        if col[1] == \"Adj_Close\":\n            df.drop(col, axis=1, inplace=True)\n            count -= 1\n        if col[1] == \"log_Adj_Close\":\n            log_adj_close_cols_ids.append(count)\n        if col[1] == \"log_Volume\":\n            volume_cols_ids.append(count)\n        if col[1] == \"log_Volatility\":\n            volatility_cols_ids.append(count)\n        if col[0] == \"weekday\":\n            weekday_col_id.append(count)\n        count += 1\n    df = df.to_numpy()\n    x, y = sliding_windows_mutli_features(df, window_size, log_adj_close_cols_ids)\n\n    # x.shape, y.shape\n    return x, y\n\ndef get_train_test(x, y, train_ratio):\n    train_size = int(len(y)*train_ratio)\n    test_size = len(y) - train_size\n\n    dataX = Variable(torch.Tensor(np.array(x)))\n    dataY = Variable(torch.Tensor(np.array(y)))\n\n    trainX = Variable(torch.Tensor(np.array(x[0:train_size])))\n    trainY = Variable(torch.Tensor(np.array(y[0:train_size])))\n\n    testX = Variable(torch.Tensor(np.array(x[train_size:len(x)])))\n    testY = Variable(torch.Tensor(np.array(y[train_size:len(y)])))\n\n    return trainX, trainY, testX, testY\n\n\ndef check_mkdir(dir_name):\n    if not os.path.exists(dir_name):\n        os.mkdir(dir_name)\n\n\ndef binary_y(y_np, criterion):\n    for i in range(len(y_np)):\n        if y_np[i] > criterion:\n            y_np[i] = int(1)\n        else:\n            y_np[i] = int(0)\n        \n    return y_np\n\n\ndef sliding_windows_single_feature(X, y, seq_length):\n    x = []\n    Y = []\n\n    for i in range((X.shape[0])-seq_length-1):\n        #change here after finishing feature engineering process\n        _x = X[i:(i+seq_length), :] \n        _y = y[i+seq_length] ## column 1 contains the labbel(log_Adj_Close)\n        x.append(_x)\n        Y.append(_y)\n\n    return np.array(x), np.array(Y)\n\ndef get_bc_per_stock_Xy(nasdaq, features, stock_name, train_ratio):\n    stock = get_stock(nasdaq, features, stock_name)\n\n    stock_new = stock.copy()\n    stock_log_adj = stock[\"log_Adj_Close\"]\n    stock_log_adj_diff = stock_log_adj.shift(-1) - stock_log_adj\n    stock_new[\"log_Adj_Close\"] = stock_log_adj_diff # tomorrow - today\n    X = stock_new\n    y = stock_new[\"log_Adj_Close\"].iloc[1:-1] #　そのまま, getting rid of nan although ind 0 is not nan for y\n    X[\"log_Adj_Close\"] = stock_new[\"log_Adj_Close\"].shift(1) # today - yesterday\n    X = X.iloc[1:-1] # getting rid of nans\n    new_index = y.index\n\n    scaler = MinMaxScaler(feature_range=(-1, 1))\n    X = scaler.fit_transform(X)\n    y = scaler.fit_transform(y.to_numpy().reshape(-1,1))\n\n    # to 1 and 0 two class classification\n    X[:, 2] = binary_y(X[:, 2], c)\n    y = binary_y(y, c)\n\n    X, y = sliding_windows_single_feature(X, y, 50)\n\n    train_size = int(X.shape[0]*train_ratio)\n    \n    X_train, X_test = X[:train_size, :, :], X[train_size:, :, :]\n    y_train, y_test = y[:train_size], y[train_size:]\n    \n    return X_train, X_test, y_train, y_test\n\n\n# def save_checkpoint(save_path, model, optimizer, valid_loss):\n\n#     if save_path == None:\n#         return\n    \n#     state_dict = {'model_state_dict': model.state_dict(),\n#                   'optimizer_state_dict': optimizer.state_dict(),\n#                   'valid_loss': valid_loss}\n    \n#     torch.save(state_dict, save_path)\n#     print(f'Model saved to ==> {save_path}')\n    \n# def save_metrics(save_path, train_loss_list, valid_loss_list, global_steps_list):\n#     if save_path == None:\n#         return\n#     state_dict = {'train_loss_list': train_loss_list,\n#                   'valid_loss_list': valid_loss_list,\n#                   'global_steps_list': global_steps_list}\n    \n#     torch.save(state_dict, save_path)\n#     print(f'Model saved to ==> {save_path}')\n    \n    \n# def load_checkpoint(load_path, model, optimizer, device=torch.device(\"cpu\")):\n\n#     if load_path==None:\n#         return\n    \n#     state_dict = torch.load(load_path, map_location=device)\n#     print(f'Model loaded from <== {load_path}')\n    \n#     model.load_state_dict(state_dict['model_state_dict'])\n#     optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n    \n#     return state_dict['valid_loss']\n\n\n# def load_metrics(load_path, device=torch.device(\"cpu\")):\n\n#     if load_path==None:\n#         return\n    \n#     state_dict = torch.load(load_path, map_location=device)\n#     print(f'Model loaded from <== {load_path}')\n    \n#     return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']\n","metadata":{"execution":{"iopub.status.busy":"2022-01-22T10:15:24.247239Z","iopub.execute_input":"2022-01-22T10:15:24.247632Z","iopub.status.idle":"2022-01-22T10:15:24.285170Z","shell.execute_reply.started":"2022-01-22T10:15:24.247588Z","shell.execute_reply":"2022-01-22T10:15:24.284109Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#################### LOAD DATA ######################\n################ DEFINE CONSTANTS ###################\n##### we will only predict for one stock here #######\n\nnasdaq = pd.read_csv(data_root + \"NASDAQ_100_Data_From_2010.csv\", sep=\"\\t\")\n\n# 'FOR APPLE, WE SET CRITERION AS 0.14'\n# for c in [0.25,0.2,0.15, 0.14, 0.1, 0.05, 0.03]:\n#     print(c)\n#     print(Counter(binary_y(list(X[2]), c)))\n#     print(Counter(binary_y(list(y), c)))\n\nc = 0.14 # this is for scaled apple stock \nwindow_size = 50\ntrain_ratio = 0.8316\nbatch_size = 381 # 366.4 is len(train)/5 if train_ratio == 0.80\nstock_name = \"AAPL\"\nDROPOUT = 0.2\nif torch.cuda.is_available():\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')\n    \n# the order of the features is extremely important for latter indexing so make sure\nfeatures = ['log_Volatility', 'log_Volume', 'log_Adj_Close']\ndirname = f\"{stock_name}_bc\"\ncheck_mkdir(dirname)\n# using = ['FB', 'TSLA', 'AAPL', 'AMZN', 'NVDA', 'MSFT', 'GOOGL']\n# AAPL(Apple), MSFT(Microsoft), GOOGL(Google), AMZN(Amazon), TSLA(Tesla), FB(Facebook), NVDA(Nvidia)\n\n# return scaled numpy array of train, test sets of one stock\nX_train, X_test, y_train, y_test = get_bc_per_stock_Xy(nasdaq, features, stock_name, train_ratio)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T10:15:24.286653Z","iopub.execute_input":"2022-01-22T10:15:24.287166Z","iopub.status.idle":"2022-01-22T10:15:25.178537Z","shell.execute_reply.started":"2022-01-22T10:15:24.287099Z","shell.execute_reply":"2022-01-22T10:15:25.177654Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"- lstm cell below","metadata":{}},{"cell_type":"code","source":"class LSTM(nn.Module):\n\n    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n        super(LSTM, self).__init__()\n        \n        self.num_classes = num_classes\n        self.num_layers = num_layers\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        #self.seq_length = seq_length\n\n        self.dropout = nn.Dropout(p=DROPOUT)\n        \n        # what does the batch_first do\n        self.lstm = nn.LSTM(\\\n            input_size=input_size, \n            hidden_size=hidden_size,\n            num_layers=num_layers, \n            batch_first=True,\n            )\n        \n        # Linear(in_features, out_features)\n        self.fc = nn.Linear(hidden_size, num_classes) \n        \n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        h_0 = Variable(torch.zeros(\n            self.num_layers, x.size(0), self.hidden_size).to(device))\n        \n        c_0 = Variable(torch.zeros(\n            self.num_layers, x.size(0), self.hidden_size).to(device))\n        \n        # Propagate input through LSTM\n        ula, (h_out, _) = self.lstm(x, (h_0, c_0))\n        \n        h_out = h_out.view(-1, self.hidden_size)\n        \n        out = self.fc(h_out)                                         \n        out = self.dropout(out)\n        \n        out = self.sigmoid(out)\n       \n        return out\n    \ndef init_weights(model):\n    for name, param in model.named_parameters():\n        nn.init.uniform_(param.data, -0.88, 0.08)\n\n# create a nn class (just-for-fun choice :-) \n# class RMSELoss(nn.Module):\n#     def __init__(self):\n#         super().__init__()\n#         self.mse = nn.MSELoss()\n        \n#     def forward(self,yhat,y):\n#         return torch.sqrt(self.mse(yhat,y))\n\n\nclass PearsonLoss(nn.Module):\n    def forward(self, x, y):\n        vx = x - torch.mean(x)\n        vy = y - torch.mean(y)\n        \nclass BCELoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.bce = nn.BCELoss()\n        \n    def forward(self, yhat, y):\n        return self.bce(yhat, y)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-22T10:15:25.179979Z","iopub.execute_input":"2022-01-22T10:15:25.180256Z","iopub.status.idle":"2022-01-22T10:15:25.196954Z","shell.execute_reply.started":"2022-01-22T10:15:25.180220Z","shell.execute_reply":"2022-01-22T10:15:25.195866Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"###### Parameters #######\nnum_epochs = 300\nlearning_rate = 1e-3\ninput_size = 3 # features\nhidden_size = 512\nnum_layers = 1 # changed from one because of dropout error -> prob diff\nnum_classes = 1 # because we are using 1 stock\n#########################\nbest_val_loss = 100    \n","metadata":{"execution":{"iopub.status.busy":"2022-01-22T10:15:25.200836Z","iopub.execute_input":"2022-01-22T10:15:25.201468Z","iopub.status.idle":"2022-01-22T10:15:25.210374Z","shell.execute_reply.started":"2022-01-22T10:15:25.201438Z","shell.execute_reply":"2022-01-22T10:15:25.209669Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"\nbest_val_loss = 100 \n### Init Model\nlstm = LSTM(num_classes, input_size, hidden_size, num_layers)\nlstm.to(device)\n\n# lstm.apply(init_weights) #is this necessary?\ntrainX, trainY = Variable(torch.Tensor(X_train)), Variable(torch.Tensor(y_train))\ntestX, testY = Variable(torch.Tensor(X_test)), Variable(torch.Tensor(y_test))","metadata":{"execution":{"iopub.status.busy":"2022-01-22T10:15:25.212391Z","iopub.execute_input":"2022-01-22T10:15:25.212617Z","iopub.status.idle":"2022-01-22T10:15:29.141279Z","shell.execute_reply.started":"2022-01-22T10:15:25.212584Z","shell.execute_reply":"2022-01-22T10:15:29.140480Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"batch_end_idxs = []\nfor idx in range(1, X_train.shape[0]+1):\n    if idx % batch_size == 0:\n        batch_end_idxs.append(idx-1)\n\n# making sure batch_end ends at end of X_train\nassert batch_end_idxs[-1] == X_train.shape[0] - 1 ","metadata":{"execution":{"iopub.status.busy":"2022-01-22T10:15:29.142786Z","iopub.execute_input":"2022-01-22T10:15:29.143075Z","iopub.status.idle":"2022-01-22T10:15:29.148486Z","shell.execute_reply.started":"2022-01-22T10:15:29.143015Z","shell.execute_reply":"2022-01-22T10:15:29.147416Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"############################################################################################\n############################################################################################\n############################### ONLY RUN FOR TRAINING ######################################\n############################################################################################\n############################################################################################\n\n        \n### Set Criterion Optimizer and scheduler\ncriterion = torch.nn.BCELoss().to(device) \noptimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate, weight_decay=1e-5)\n\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=100, factor=0.5, min_lr=1e-7, eps=1e-08)\n\n#optimizer = torch.optim.SGD(lstm.parameters(), lr=learning_rate)\n\n\noptimizer.zero_grad() # moved this out of loop \nvall_losses = []\ntrain_losses = []\n\n# Train model\n############### \nbatch_start_idx = 0\nfor epoch in progress_bar(range(num_epochs)):\n    lstm.train()\n    \n    loss_per_batch = []\n    for batch_end_idx in batch_end_idxs:\n        trainX, trainY = trainX[batch_start_idx:batch_end_idx, : , :], trainY[batch_start_idx:batch_end_idx]\n#         print(trainX.shape, trainY.shape)\n        \n        outputs= lstm(trainX.to(device))\n        torch.nn.utils.clip_grad_norm_(lstm.parameters(),1)\n\n        # obtain loss func\n        loss = criterion(outputs, trainY.to(device))\n        loss_per_batch.append(loss.detach().cpu().item())\n        loss.backward()\n\n        scheduler.step(loss)\n        optimizer.step()\n        \n    loss_per_batch_mean = np.mean(np.array(loss_per_batch))\n    train_losses.append(loss_per_batch_mean)\n\n    #evaluate on test\n    lstm.eval()\n    valid = lstm(testX.to(device))\n    vall_loss = criterion(valid, testY.to(device))\n    vall_losses.append(vall_loss)\n\n    scheduler.step(vall_loss)\n    \n    \n\n    if vall_loss.cpu().item() < best_val_loss:\n        torch.save(lstm.state_dict(), dirname + \"/\" + f'best_model.pt')\n        print(\"saved model epoch:\",epoch,\"val loss is:\",vall_loss.cpu().item())\n        best_val_loss = vall_loss.cpu().item()\n\n    if epoch % 50 == 0:\n        print(f\"Epoch: {epoch}, loss: {loss_per_batch_mean}, valid loss:{vall_loss.cpu().item()}\")","metadata":{"execution":{"iopub.status.busy":"2022-01-22T10:15:29.150065Z","iopub.execute_input":"2022-01-22T10:15:29.150612Z","iopub.status.idle":"2022-01-22T10:16:10.438539Z","shell.execute_reply.started":"2022-01-22T10:15:29.150570Z","shell.execute_reply":"2022-01-22T10:16:10.436870Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20, 8))\nplt.plot(vall_losses)\nplt.plot(train_losses)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T10:16:10.439366Z","iopub.status.idle":"2022-01-22T10:16:10.439659Z","shell.execute_reply.started":"2022-01-22T10:16:10.439502Z","shell.execute_reply":"2022-01-22T10:16:10.439519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef save_checkpoint(save_path, model, optimizer, valid_loss):\n\n    if save_path == None:\n        return\n    \n    state_dict = {'model_state_dict': model.state_dict(),\n                  'optimizer_state_dict': optimizer.state_dict(),\n                  'valid_loss': valid_loss}\n    \n    torch.save(state_dict, save_path)\n    print(f'Model saved to ==> {save_path}')","metadata":{"execution":{"iopub.status.busy":"2022-01-22T10:16:10.440918Z","iopub.status.idle":"2022-01-22T10:16:10.441308Z","shell.execute_reply.started":"2022-01-22T10:16:10.441101Z","shell.execute_reply":"2022-01-22T10:16:10.441122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot predictions\npred = lstm(testX.to(device))\npred_cpu = pred.detach().cpu().numpy()\ntestY_cpu = testY.detach().cpu().numpy()\nscaler = StandardScaler()\npred_cpu = scaler.fit_transform(pred_cpu)\ntestY_cpu = scaler.fit_transform(testY_cpu)\nplt.plot(pred_cpu)\nplt.plot(testY_cpu)\nplt.legend([\"pred\", \"gt\"])\nplt.show()\n# plt.xlim(0, 50)\n# plt.ylim((0.3, 0.45))\n\n# coefs\nnp.corrcoef(pred_cpu.squeeze(), testY_cpu.squeeze())","metadata":{"execution":{"iopub.status.busy":"2022-01-22T10:16:10.442230Z","iopub.status.idle":"2022-01-22T10:16:10.442600Z","shell.execute_reply.started":"2022-01-22T10:16:10.442385Z","shell.execute_reply":"2022-01-22T10:16:10.442406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def data_train_results(lstm, nasdaq, features, stock_name):\n    X_train, X_test, y_train, y_test = get_bc_per_stock_Xy(nasdaq, features, stock_name)\n    # lstm.apply(init_weights) #is this necessary?\n    trainX, trainY = Variable(torch.Tensor(X_train)), Variable(torch.Tensor(y_train))\n    testX, testY = Variable(torch.Tensor(X_test)), Variable(torch.Tensor(y_test))\n\n    \n\n    ### Set Criterion Optimizer and scheduler\n    criterion = torch.nn.BCELoss().to(device) \n    optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate, weight_decay=1e-5)\n\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=100, factor=0.5, min_lr=1e-7, eps=1e-08)\n\n    #optimizer = torch.optim.SGD(lstm.parameters(), lr=learning_rate)\n\n    optimizer.zero_grad()\n    # Train model\n\n\n    ############### \n    for epoch in progress_bar(range(num_epochs)):\n        lstm.train()\n        outputs= lstm(trainX.to(device))\n        torch.nn.utils.clip_grad_norm_(lstm.parameters(),1)\n\n        # obtain loss func\n        loss = criterion(outputs, trainY.to(device))\n        loss.backward()\n\n        scheduler.step(loss)\n        optimizer.step()\n\n        #evaluate on test\n        lstm.eval()\n        valid = lstm(testX.to(device))\n        vall_loss = criterion(valid, testY.to(device))\n\n        scheduler.step(vall_loss)\n        \n        model_directory = check_mkdir(f\"{stock_name}_models\")\n        if vall_loss.cpu().item() < best_val_loss:\n            torch.save(lstm.state_dict(), model_directory + \"/\" + f'{stock_name}_best_model_{epoch}.pt')\n            print(\"saved best model epoch:\",epoch,\"val loss is:\",vall_loss.cpu().item())\n            best_val_loss = vall_loss.cpu().item()\n\n        if epoch % 50 == 0:\n            print(f\"Epoch: {epoch}, loss: {loss.cpu().item()}, valid loss:{vall_loss.cpu().item()}\")\n","metadata":{"execution":{"iopub.status.busy":"2022-01-22T10:16:10.443722Z","iopub.status.idle":"2022-01-22T10:16:10.444336Z","shell.execute_reply.started":"2022-01-22T10:16:10.444105Z","shell.execute_reply":"2022-01-22T10:16:10.444131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}